{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "from statsmodels.sandbox.regression.gmm import GMM\n",
    "from statsmodels.base.model import GenericLikelihoodModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.genfromtxt('data2.dat', delimiter='  ')\n",
    "data2 = data2.transpose()\n",
    "\n",
    "y = data2[0]\n",
    "x = sm.add_constant(data2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.381592\n",
      "         Iterations: 202\n",
      "         Function evaluations: 360\n",
      "                                part_b Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   Log-Likelihood:                -138.16\n",
      "Model:                         part_b   AIC:                             278.3\n",
      "Method:            Maximum Likelihood   BIC:                             280.9\n",
      "Date:                Thu, 27 Sep 2018                                         \n",
      "Time:                        17:35:31                                         \n",
      "No. Observations:                 100                                         \n",
      "Df Residuals:                      99                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta_1        3.0659      0.187     16.379      0.000       2.699       3.433\n",
      "theta_2        1.0272      0.229      4.490      0.000       0.579       1.476\n",
      "sigma          0.9633      0.068     14.142      0.000       0.830       1.097\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "#part a - MLE\n",
    "\n",
    "\n",
    "class part_b(GenericLikelihoodModel):\n",
    "    \"\"\"class for evaluating question 2 part a\"\"\"\n",
    "    \n",
    "    def nloglikeobs(self, params):\n",
    "        t1, t2, sigma = params\n",
    "        endog, exog = self.endog, self.exog.squeeze()\n",
    "        eps = np.log(endog) - t1 - exog**t2\n",
    "        #calculate log likelihood\n",
    "        return -norm(0,sigma).logpdf(eps).sum()\n",
    "\n",
    "    \n",
    "    def fit(self, start_params=None, maxiter=10000, maxfun=5000, **kwds):\n",
    "        if start_params == None:\n",
    "            start_params = [.5, .5,.5]\n",
    "        return super(part_b, self).fit(start_params=start_params,\n",
    "                                       maxiter=maxiter, maxfun=maxfun, **kwds)\n",
    "\n",
    "    \n",
    "model_b = part_b(data2[0],data2[1])\n",
    "result_b = model_b.fit()\n",
    "print(result_b.summary(xname=['theta_1', 'theta_2', 'sigma']))\n",
    "\n",
    "\n",
    "#sources (same as last time):\n",
    "#http://www.statsmodels.org/0.6.1/examples/notebooks/generated/generic_mle.html\n",
    "#http://rlhick.people.wm.edu/posts/estimating-custom-mle.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 61\n",
      "         Function evaluations: 119\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 23\n",
      "         Function evaluations: 46\n",
      "                                part_c Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   Hansen J:                    1.456e-09\n",
      "Model:                         part_c   Prob (Hansen J):                   nan\n",
      "Method:                           GMM                                         \n",
      "Date:                Thu, 27 Sep 2018                                         \n",
      "Time:                        17:35:31                                         \n",
      "No. Observations:                 100                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta_1        3.0574      0.186     16.425      0.000       2.693       3.422\n",
      "theta_2        1.0394      0.216      4.814      0.000       0.616       1.463\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "#part c - GMM\n",
    "\n",
    "class part_c(GMM):\n",
    "    \"\"\"class for evaluating question 2 part b\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwds):\n",
    "        # set appropriate counts for moment conditions and parameters\n",
    "        kwds.setdefault('k_moms', 2)\n",
    "        kwds.setdefault('k_params',2)\n",
    "        super(part_c, self).__init__(*args, **kwds)\n",
    "    \n",
    "    \n",
    "    def fit(self, start_params=None, maxiter=10000, **kwds):\n",
    "        if start_params == None:\n",
    "            start_params = np.array([.5, .5])\n",
    "        return super(part_c, self).fit(start_params=start_params,\n",
    "                                       maxiter=maxiter,  **kwds)\n",
    "    \n",
    "        \n",
    "    def momcond(self, params):\n",
    "        t1,t2 = params  \n",
    "        endog, exog = self.endog, self.exog.squeeze()\n",
    "        #different model, essentially the same since its an injective mapping\n",
    "        eps =  np.log(endog) - t1 - exog**t2 \n",
    "        g = np.column_stack( (eps, eps*exog ))\n",
    "        return g \n",
    "\n",
    "    \n",
    "model_c = part_c(data2[0],data2[1], None)\n",
    "result_c = model_c.fit(maxiter=2, optim_method='nm', wargs=dict(centered=False))\n",
    "print(result_c.summary(xname=['theta_1', 'theta_2']))\n",
    "\n",
    "\n",
    "#sources (same as last time):\n",
    "#https://github.com/josef-pkt/misc/blob/master/notebooks/ex_gmm_gamma.ipynb\n",
    "#https://www.statsmodels.org/dev/generated/statsmodels.sandbox.regression.gmm.GMM.html#statsmodels.sandbox.regression.gmm.GMM\n",
    "#https://gist.github.com/josef-pkt/6895915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
