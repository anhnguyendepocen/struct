{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Assignment 3: Structural Econometrics\n",
    "## Eric Schulman\n",
    "\n",
    "Solutions to ECO 388E assignment 3 at the University of Texas written by Eric Schulman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.interpolate import interp1d #pre written interpolation function\n",
    "from statsmodels.base.model import GenericLikelihoodModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "The value for a machine of age $a_t$ is\n",
    "\n",
    "$$ V(a_t, \\epsilon_t; \\theta) = \\underset{i_t}{max} E[\\sum_{\\tau=t}^\\infty \\beta^{\\tau-t}\\pi(a_\\tau, i_\\tau, \\epsilon_{\\tau}, ;\\theta)|a_t; \\theta] $$ \n",
    "\n",
    "\n",
    "As a result we can write a Bellman equation for the firm maximizing future profits is expressed below:\n",
    "\n",
    "$$V(a_t,i_t,\\epsilon_{0t},\\epsilon_{1t}) = \\pi(a_t, i_t, \\epsilon_{1t}, \\epsilon_{0t}) + max_{i_{t+1}} \\beta E(V(a_{t+1},i_{t+1},\\epsilon_{0t+1},\\epsilon_{1t+1}) |a_t, i_t; \\theta)$$\n",
    "\n",
    "Because of the conditional independence assumption $\\epsilon_{it}$ is not serially correlated.\n",
    "\n",
    "### Part 2\n",
    "\n",
    "In class, $x_t$ was continuous. Here $a_t$ is discrete. Additionally, we did not parameterize $c(x_t, \\theta)$. Here $c(a_t,\\theta) = \\theta a_t$, As a result, $c(0,\\theta) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "The code below is designed to calculate the value function using forward recursion. We determine the initial value using a contraction mapping iterating forward until it converges.\n",
    "\n",
    "The value function is a 5x2 array. The rows contain the values when the state is $a_t$. The columns contain the value based on $i_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = .9\n",
    "GAMMA = .5772 #euler's constant\n",
    "\n",
    "def value_helper(a, theta1, cost, v_init):\n",
    "    \"\"\"helper function for calculating value function.\n",
    "    \n",
    "    given the value for the first period, calculate forward \n",
    "    'a' periods\"\"\"\n",
    "    if a <= 0: #initial period\n",
    "        return  v_init  \n",
    "    else:\n",
    "        v_next = value_helper(a-1, theta1, cost, v_init)\n",
    "        v_0 = a*theta1 + BETA*(GAMMA + np.log( np.exp(v_next[-1][0]) + np.exp(v_next[-1][1]) ))  \n",
    "        v_1 = cost + BETA*(GAMMA + np.log(  np.exp(v_next[-1][0]) + np.exp(v_next[-1][1]) )) \n",
    "        \n",
    "        return np.concatenate( ( v_next,[[v_0,v_1]]) )\n",
    "\n",
    "\n",
    "v = value_helper(5, -1, -3, [[0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_function(a, theta1, cost, v_init, error, maxiter):\n",
    "    \"\"\"solve for the first period of the value function\n",
    "    with the contraction mapping loop\n",
    "    \n",
    "    You can choose how far into the future it goes by setting max_periods\"\"\"\n",
    "    \n",
    "    #only need to iterate 1 periods into the future\n",
    "    v = value_helper(1, theta1, cost, v_init)\n",
    "    \n",
    "    #stop iterating when the last two periods look the same\n",
    "    while ( maxiter >= 0  \n",
    "           and ( abs(  v[1,0] - v[0,0] )  > error\n",
    "           or abs(  v[1,1] - v[0,1] )  > error) ):\n",
    "        \n",
    "        #recompute value function until convergence\n",
    "        return value_function(a, theta1, cost, [v[1,:]], error, maxiter-1)  \n",
    "    \n",
    "    return value_helper(a, theta1, cost, [v[1,:]])[1:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "* Below I solve the model when $\\theta_1 = -1$ and $R = -3$. \n",
    "\n",
    "* When $a_t = 2$, we can see that $V(2,0) - V(2,1) = 1$ so the firm chooses not to replace the engine. If the value of $\\epsilon_{0t} - \\epsilon_{1t}$ exceeds 1, then the firm will choose to replace the engine in period 2.\n",
    "\n",
    "* I calculate the probability of this difference below using the exterme value distribution.\n",
    "\n",
    "* Below I also calulate the value function when $a_t = 4$, $\\epsilon_{0t} = 1$ and $\\epsilon_{1t} =-1.5$. It is still cheaper to replace this period than to wait until period 5 to replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Value Function:\n",
      "           0         1\n",
      "0  -3.655248 -5.655248\n",
      "1  -4.656008 -5.656008\n",
      "2  -6.388992 -6.388992\n",
      "3  -8.606780 -7.606780\n",
      "4 -11.044687 -9.044687\n",
      "\n",
      "2. V(2,0) - V(2,1) = 1.0\n",
      "\n",
      "3. Likelihood: 0.2689414213699951\n",
      "\n",
      "4. PDV: -9.74610218495787\n"
     ]
    }
   ],
   "source": [
    "v = value_function(5, -1, -3, [[0,0]] , .001 , 100)\n",
    "print '1. Value Function:'\n",
    "print pd.DataFrame(v)\n",
    "\n",
    "#difference between e0 and e1\n",
    "diff = v[1,0] - v[1,1]\n",
    "print '\\n2. V(2,0) - V(2,1) = %s'%diff\n",
    "\n",
    "#probability of this different\n",
    "print '\\n3. Likelihood: %s'%( np.exp(-diff)/(1+np.exp(-diff)) )\n",
    "\n",
    "#PDV a = 4, e0= 1, e1=-1.5\n",
    "print '\\n4. PDV: %s'%np.maximum(-3 + 1 + BETA*v[3,0], -4 + -1.5 +BETA*v[3,1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - 6 \n",
    "\n",
    "Below I calculate the value of $\\theta_1$ and $R$ and standard errors. \n",
    "\n",
    "The likelihood of $i_t$ is conditional on $a_t$ because my decision to replace this period depends on how old the engine is. The expected future costs, which I base my decision on, depend on the current age of the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into memory for part 5\n",
    "data = np.loadtxt(\"data.asc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.425320\n",
      "         Iterations: 55\n",
      "         Function evaluations: 104\n",
      "                                 Rust Results                                 \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   Log-Likelihood:                -425.32\n",
      "Model:                           Rust   AIC:                             852.6\n",
      "Method:            Maximum Likelihood   BIC:                             857.5\n",
      "Date:                Thu, 20 Dec 2018                                         \n",
      "Time:                        12:29:17                                         \n",
      "No. Observations:                1000                                         \n",
      "Df Residuals:                     999                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta_1       -1.3833      0.082    -16.843      0.000      -1.544      -1.222\n",
      "R             -3.3137      0.219    -15.115      0.000      -3.743      -2.884\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "class Rust(GenericLikelihoodModel):\n",
    "    \"\"\"class for estimating the values of R and theta\"\"\"\n",
    "    \n",
    "    def nloglikeobs(self, params, v=False):\n",
    "        \n",
    "        theta1, R = params\n",
    "        \n",
    "        # Input our data into the model\n",
    "        i = self.exog[:,0] #reshape\n",
    "        a = self.endog.astype(int)\n",
    "        \n",
    "        #solve value function based on params\n",
    "        v = value_function(5, theta1, R, [[0,0]] , .01 , 100)\n",
    "        \n",
    "        #interpolate using scipy (easier than indexing)\n",
    "        v0 = interp1d(range(1,6), v[:,0],fill_value=\"extrapolate\")\n",
    "        v1 = interp1d(range(1,6), v[:,1],fill_value=\"extrapolate\")\n",
    "        \n",
    "        diff = v1(a) - v0(a)\n",
    "    \n",
    "        #calculate likelihood of each obs\n",
    "        pr0 = 1/(1+np.exp(diff))\n",
    "        pr1 = np.exp(diff)/(1+np.exp(diff))\n",
    "\n",
    "        likelihood = (1-i)*pr0 + i*pr1\n",
    "        return -np.log(likelihood).sum()\n",
    "    \n",
    "    \n",
    "    def fit(self, start_params=None, maxiter=1000, maxfun=5000, **kwds):\n",
    "        if start_params == None:\n",
    "            start_params = [1,1]\n",
    "        return super(Rust, self).fit(start_params=start_params,\n",
    "                                       maxiter=maxiter, maxfun=maxfun, **kwds)\n",
    "    \n",
    "    \n",
    "model = Rust(data[:,0],data[:,1])\n",
    "\n",
    "result = model.fit()\n",
    "print(result.summary(xname=['theta_1', 'R']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7\n",
    "\n",
    "#### Section A\n",
    "\n",
    "To accomodate $\\theta_{1A}$ and $\\theta_{1B}$ we would need to modify the dynamic program as follows\n",
    "\n",
    "$$V_j(a_{t},i_t,\\epsilon_{it}) = \\pi(a_t, i_t, \\epsilon_{jt}; \\theta_j) + max_{i_{t+1}} \\beta E(V_j(a_{t+1},i_{t+1},\\epsilon_{jt+1}) |a_t, \\epsilon_{it}, i_t; \\theta_j)$$\n",
    "\n",
    "Now, there is an $j$ subscript on $\\theta$. In principle, this means we will have to calculate two different value functions using the contraction mapping. The likelihood of replacing or not $i_j$. Using the extreme value distribution and conditional independence. This is given by:\n",
    "\n",
    "\n",
    "$$Pr(i_j =1 | a_j, \\epsilon_{jt} ) = \\alpha \\dfrac{e^{V_{1A}(a_j, \\epsilon_{jt})}}{e^{V_{0A}(a_j, \\epsilon_{jt})} + e^{V_{1A}(a_j, \\epsilon_{jt})}} + (1-\\alpha) \\dfrac{e^{V_{1B}(a_j, \\epsilon_{jt})}}{e^{V_{0B}(a_j, \\epsilon_{jt})} + e^{V_{1B}(a_j, \\epsilon_{jt})}}$$\n",
    "\n",
    "#### Section B\n",
    "\n",
    "We must now consider the likelihood of a sequence of decisions $ \\{i(a_{jt},\\epsilon_{jt}) = 1\\}_{t<T}$. \n",
    "\n",
    "$$Pr( \\{i(a_{jt},\\epsilon_{jt}) = 1\\}_{t<T} ) = \\alpha Pr(\\{i(a_{jt},\\epsilon_{jt}) = 1\\}_{t<T} ; \\theta_{1A} ) + (1-\\alpha) Pr(\\{i(a_{jt},\\epsilon_{jt}) = 1\\}_{t<T} ; \\theta_{1B} ) $$\n",
    "\n",
    "\n",
    "Under conditional indpendence, and the extreme value distribution we would calculate the following:\n",
    "\n",
    "\n",
    "$$Pr( \\{i(a_{jt},\\epsilon_{jt}) = 1\\}_{t<T}) = \\alpha \\prod_{t<T} \\dfrac{e^{V_{1A}(a_j, \\epsilon_{jt})}}{e^{V_{0A}(a_j, \\epsilon_{jt})} + e^{V_{1A}(a_j, \\epsilon_{jt})}} + (1-\\alpha) \\prod_{t<T} \\dfrac{e^{V_{1B}(a_j, \\epsilon_{jt})}}{e^{V_{0B}(a_j, \\epsilon_{jt})} + e^{V_{1B}(a_j, \\epsilon_{jt})}}$$\n",
    "\n",
    "#### Section C\n",
    "\n",
    "If machines differ, then $\\epsilon_{jt}$ are now serially correlated. We must make $\\epsilon_{jt}$ a state variable in our expected value function. \n",
    "\n",
    "Before we had $\\bar{V}(a_t, \\epsilon_{it}) = E(V_j(a_{t+1},\\epsilon_{jt+1}) |a_t ; \\theta_j)$.\n",
    "\n",
    "Now we need to condition on $\\epsilon_{jt}$ i.e. $\\bar{V}(a_t, \\epsilon_{it}) = E(V_j(a_{t+1},\\epsilon_{jt+1}) |a_t, \\epsilon_{jt} ; \\theta_j)$\n",
    "\n",
    "#### Section D\n",
    "\n",
    "The initial conditions problem involves the fact that we do not see the initial period. As a result, any serial correlation can be caused by unobserved heterogeneity or by the initial draw of $\\epsilon_{jt}$.\n",
    "\n",
    "We can solve the problem by simulating $\\epsilon_{jt}$ all the way back to the initial period.\n",
    "\n",
    "#### Section E\n",
    "\n",
    "For unobserved reasons firms will not replace their engines as often as expected in the data. If firms systematically wait longer than expected to replace their engines, $\\lambda$ will be identified. More specifically, the distribution of $\\epsilon_{it}$, the unobservables, will have a mean given by the extreme value distribution. The difference between this mean (in theory) and its value in practice will identify $\\lambda$.\n",
    "\n",
    "### Part 8\n",
    "\n",
    "Below I estimate the model using the Hotz-Miller algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22344462],\n",
       "       [0.52314252],\n",
       "       [0.77256052],\n",
       "       [0.90702516],\n",
       "       [0.96366077]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hm_initial_pr(a_obs, i_obs):\n",
    "    \"\"\"calculate state pr\"\"\"\n",
    "    \n",
    "    df = np.array([a_obs,i_obs]).transpose()\n",
    "    df = pd.DataFrame(df, columns=('a','i'))\n",
    "    pr_obs = df.groupby('a')\n",
    "    pr_obs = pr_obs.sum()/(1.*pr_obs .count())\n",
    "\n",
    "    return  np.array(pr_obs)\n",
    "    \n",
    "\n",
    "def hm_transitions(a_max):\n",
    "    \"\"\"calculate transitions, deterministic\n",
    "    in this case\"\"\"\n",
    "    \n",
    "    trans1 = np.zeros((a_max,a_max))\n",
    "    trans1[:,0] = np.ones(a_max)\n",
    "    \n",
    "    trans0 = np.vstack( (np.identity(a_max-1), np.zeros(a_max-1)))\n",
    "    trans0 = np.hstack( ( np.zeros((a_max,1)), trans0 ))\n",
    "    trans0[a_max-1][a_max-1] = 1\n",
    "\n",
    "    return trans0,trans1\n",
    "\n",
    "\n",
    "def hm_value(a_max, theta1, cost, pr_obs):\n",
    "    \"\"\"calculate value function using hotz miller approach\"\"\"\n",
    "    \n",
    "    #set up matrices, transition is deterministic\n",
    "    trans0, trans1 = hm_transitions(a_max)\n",
    "    a = np.arange(1,a_max+1).reshape(a_max,1)\n",
    "    \n",
    "    #calculate value function for all state\n",
    "    pr_tile = np.tile( pr_obs.reshape(a_max,1), (1,a_max))\n",
    "    \n",
    "    denom = (np.identity(a_max) - BETA*(1-pr_tile)*trans0 - BETA*trans1*pr_tile)\n",
    "        \n",
    "    numer = ( (1-pr_obs)*(theta1*a  + GAMMA - np.log(1-pr_obs)) + \n",
    "                 pr_obs*(cost+ GAMMA - np.log(pr_obs) ) )\n",
    "    \n",
    "    value = np.linalg.inv(denom).dot(numer)\n",
    "    return value\n",
    "\n",
    "\n",
    "def hm_prob(a_max, theta1, cost, pr_obs):\n",
    "    \"\"\"calculate kappa using value function\"\"\"\n",
    "    \n",
    "    value = hm_value(a_max, theta1, cost, pr_obs)\n",
    "    trans0,trans1 = hm_transitions(a_max)\n",
    "    a = np.arange(1,a_max+1).reshape(a_max,1)\n",
    "\n",
    "    delta1 = np.exp( cost + BETA*trans1.dot(value))\n",
    "    delta0 = np.exp( a*theta1 + BETA*trans0.dot(value) )\n",
    "    \n",
    "    return delta1/(delta1+delta0)\n",
    "\n",
    "hm_value(5, -1, -3, hm_initial_pr(data[:,0], data[:,1]))\n",
    "hm_prob(5, -1, -3, hm_initial_pr(data[:,0], data[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I estimate the parameters using the Hotz Miller estimation routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " theta_1:-1.1495, R:-4.4537\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "class HotzMiller():\n",
    "    \"\"\"class for estimating the values of R and theta\"\"\"\n",
    "    \n",
    "    def __init__(self, a_max, a, i):\n",
    "        self.a_max = a_max\n",
    "        self.pr_obs = hm_initial_pr(a,i)\n",
    "        self.a = a\n",
    "        self.i = i\n",
    "        self.theta1 = 0\n",
    "        self.R = 0\n",
    "        \n",
    "        \n",
    "    def likelihood(self, params): \n",
    "        theta1, R = params\n",
    "        \n",
    "        # Input our data into the model\n",
    "        i = self.i\n",
    "        a = self.a.astype(int)\n",
    "        \n",
    "        #set up hm state pr\n",
    "        prob = hm_prob(self.a_max, theta1, R, self.pr_obs).transpose()[0]\n",
    "        prob = interp1d(range(1,self.a_max+1), prob,fill_value=\"extrapolate\")\n",
    "        \n",
    "        log_likelihood = (1-i)*np.log(1-prob(a)) + i*np.log(prob(a))\n",
    "        \n",
    "        return -log_likelihood.sum()\n",
    "    \n",
    "    \n",
    "    def fit(self):\n",
    "        result = minimize(self.likelihood, [-1,-3], method = 'Nelder-Mead', options={'disp': False})\n",
    "        self.theta1, self.R = result.x\n",
    "\n",
    "model_hm = HotzMiller(5, data[:,0],data[:,1])\n",
    "model_hm.fit()\n",
    "\n",
    "print '\\n theta_1:%s, R:%s'%(round(model_hm.theta1,4) , round(model_hm.R,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I write code to iterate the Hotz Miller estimation procedure 3 times. Although the parameters get closer to their maximum likelihood estimates, they rate at which it converges is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " theta_1:-1.1495, R:-4.4537\n",
      "\n",
      " Iterating 3 times ...\n",
      "\n",
      " theta_1:-1.1484, R:-4.4464\n"
     ]
    }
   ],
   "source": [
    "class AM( HotzMiller):\n",
    "    \"\"\"A class for doing the AM contraction mapping\"\"\"\n",
    "    \n",
    "    def iterate(self,numiter):\n",
    "        i = 0\n",
    "        self.fit() \n",
    "        while(i < numiter):\n",
    "            #update pr_obs based on parameters\n",
    "            self.pr_obs = hm_prob(self.a_max, self.theta1, self.R, self.pr_obs)\n",
    "            #refit the model\n",
    "            self.fit()\n",
    "            i = i +1\n",
    "    \n",
    "\n",
    "model_am = AM(5,data[:,0],data[:,1])\n",
    "#print model_am.result.summary(xname=['theta_1', 'R'])\n",
    "model_am.iterate(0)\n",
    "print '\\n theta_1:%s, R:%s'%(round(model_am.theta1,4) , round(model_am.R,4))\n",
    "n = 3\n",
    "print'\\n Iterating %s times ...'%n\n",
    "model_am.iterate(3)\n",
    "print '\\n theta_1:%s, R:%s'%(round(model_am.theta1,4) , round(model_am.R,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
