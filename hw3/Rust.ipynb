{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Assignment 3: Structural Econometrics\n",
    "## Eric Schulman\n",
    "\n",
    "Solutions to ECO 388E assignment 3 at the University of Texas written by Eric Schulman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.interpolate import interp1d #pre written interpolation function\n",
    "from statsmodels.base.model import GenericLikelihoodModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "The dynamic program for the firm maximizing future profits is expressed below:\n",
    "\n",
    "$$V(a_t,i_t,\\epsilon_{0t},\\epsilon_{1t}) = \\pi(a_t, i_t, \\epsilon_{1t}, \\epsilon_{0t}) + max_{i_{t+1}} \\beta E(V(a_{t+1},i_{t+1},\\epsilon_{0t+1},\\epsilon_{1t+1}) |a_t, i_t; \\theta)$$\n",
    "\n",
    "Because of the conditional independence assumption $\\epsilon_{it}$ is not serially correlated.\n",
    "\n",
    "### Part 2\n",
    "\n",
    "In class, $x_t$ was continuous. Here $a_t$ is discrete. Additionally, we did not parameterize $c(x_t, \\theta)$. Here $c(a_t,\\theta) = \\theta a_t$, As a result, $c(0,\\theta) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "The code below is designed to calculate the value function using forward recursion. We determine the initial value using a contraction mapping iterating forward until it converges.\n",
    "\n",
    "The value function is a 5x2 array. The rows contain the values when the state is $a_t$. The columns contain the value based on $i_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA = .9\n",
    "GAMMA = .5772 #euler's constant\n",
    "\n",
    "def value_helper(a, theta1, cost, v_init):\n",
    "    \"\"\"helper function for calculating value function.\n",
    "    \n",
    "    given the value for the first period, calculate forward \n",
    "    'a' periods\"\"\"\n",
    "    if a <= 0: #we have reached the last period\n",
    "        return  v_init  \n",
    "    else:\n",
    "        v_next = value_helper(a-1, theta1, cost, v_init)\n",
    "        v_0 = a*theta1 + BETA*(GAMMA + np.log( np.exp(v_next[-1][0]) + np.exp(v_next[-1][1]) ))  \n",
    "        v_1 = cost + BETA*(GAMMA + np.log(  np.exp(v_next[-1][0]) + np.exp(v_next[-1][1]) )) \n",
    "        \n",
    "        return np.concatenate( ( v_next,[[v_0,v_1]]) )\n",
    "\n",
    "\n",
    "v = value_helper(5, -1, -3, [[0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.65524822  -5.65524822]\n",
      " [ -4.65600819  -5.65600819]\n",
      " [ -6.38899185  -6.38899185]\n",
      " [ -8.60678021  -7.60678021]\n",
      " [-11.04468667  -9.04468667]]\n"
     ]
    }
   ],
   "source": [
    "def value_function(a, theta1, cost, v_init, error, maxiter):\n",
    "    \"\"\"solve for the first period of the value function\n",
    "    with the contraction mapping loop\n",
    "    \n",
    "    You can choose how far into the future it goes by setting max_periods\"\"\"\n",
    "    \n",
    "    #only need to iterate 1 periods into the future\n",
    "    v = value_helper(1, theta1, cost, v_init)\n",
    "    \n",
    "    #stop iterating when the last two periods look the same\n",
    "    while ( maxiter >= 0  \n",
    "           and ( abs(  v[1,0] - v[0,0] )  > error\n",
    "           or abs(  v[1,1] - v[0,1] )  > error) ):\n",
    "        \n",
    "        #recompute value function until convergence\n",
    "        return value_function(a, theta1, cost, [v[1,:]], error, maxiter-1)  \n",
    "    \n",
    "    return value_helper(a, theta1, cost, [v[1,:]])[1:,:]\n",
    "\n",
    "\n",
    "print value_function(5, -1, -3, [[0,0]],.001, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "* Below I solve the model when $\\theta_1 = -1$ and $R = -3$. \n",
    "\n",
    "* When $a_t = 2$, we can see that $V(2,0) - V(2,1) = 1$ so the firm chooses not to replace the engine. If the value of $\\epsilon_{0t} - \\epsilon_{1t}$ exceeds 1, then the firm will choose to replace the engine in period 2.\n",
    "\n",
    "* I calculate the probability of this difference below using the exterme value distribution.\n",
    "\n",
    "* Below I also calulate the value function when $a_t = 4$, $\\epsilon_{0t} = 1$ and $\\epsilon_{1t} =-1.5$. It is still cheaper to replace this period than to wait until period 5 to replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Value Function:\n",
      "           0         1\n",
      "0  -3.655248 -5.655248\n",
      "1  -4.656008 -5.656008\n",
      "2  -6.388992 -6.388992\n",
      "3  -8.606780 -7.606780\n",
      "4 -11.044687 -9.044687\n",
      "\n",
      "2. V(2,0) - V(2,1) = 1.0\n",
      "\n",
      "3. Likelihood: 0.2689414213699951\n",
      "\n",
      "4. PDV: -9.74610218495787\n"
     ]
    }
   ],
   "source": [
    "v = value_function(5, -1, -3, [[0,0]] , .001 , 100)\n",
    "print '1. Value Function:'\n",
    "print pd.DataFrame(v)\n",
    "\n",
    "#difference between e0 and e1\n",
    "diff = v[1,0] - v[1,1]\n",
    "print '\\n2. V(2,0) - V(2,1) = %s'%diff\n",
    "\n",
    "#probability of this different\n",
    "print '\\n3. Likelihood: %s'%( np.exp(-diff)/(1+np.exp(-diff)) )\n",
    "\n",
    "#PDV a = 4, e0= 1, e1=-1.5\n",
    "print '\\n4. PDV: %s'%np.maximum(-3 + 1 + BETA*v[3,0], -4 + -1.5 +BETA*v[3,1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - 6 \n",
    "\n",
    "Below I calculate the value of $\\theta_1$ and $R$ and standard errors. \n",
    "\n",
    "The likelihood of $i_t$ is conditional on $a_t$ because my decision to replace this period depends on how old the engine is. The expected future costs, which I base my decision on, depend on the current age of the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into memory for part 5\n",
    "data = np.loadtxt(\"data.asc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.425320\n",
      "         Iterations: 55\n",
      "         Function evaluations: 104\n",
      "                                 Rust Results                                 \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   Log-Likelihood:                -425.32\n",
      "Model:                           Rust   AIC:                             852.6\n",
      "Method:            Maximum Likelihood   BIC:                             857.5\n",
      "Date:                Tue, 18 Dec 2018                                         \n",
      "Time:                        14:59:35                                         \n",
      "No. Observations:                1000                                         \n",
      "Df Residuals:                     999                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta_1       -1.3833      0.082    -16.843      0.000      -1.544      -1.222\n",
      "R             -3.3137      0.219    -15.115      0.000      -3.743      -2.884\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "class Rust(GenericLikelihoodModel):\n",
    "    \"\"\"class for estimating the values of R and theta\"\"\"\n",
    "    \n",
    "    def nloglikeobs(self, params, v=False):\n",
    "        \n",
    "        theta1, R = params\n",
    "        \n",
    "        # Input our data into the model\n",
    "        i = self.exog[:,0] #reshape\n",
    "        a = self.endog.astype(int)\n",
    "        \n",
    "        #solve value function based on params\n",
    "        v = value_function(5, theta1, R, [[0,0]] , .01 , 100)\n",
    "        \n",
    "        #interpolate using scipy (easier than indexing)\n",
    "        v0 = interp1d(range(1,6), v[:,0],fill_value=\"extrapolate\")\n",
    "        v1 = interp1d(range(1,6), v[:,1],fill_value=\"extrapolate\")\n",
    "        \n",
    "        diff = v1(a) - v0(a)\n",
    "    \n",
    "        #calculate likelihood of each obs\n",
    "        pr0 = 1/(1+np.exp(diff))\n",
    "        pr1 = np.exp(diff)/(1+np.exp(diff))\n",
    "\n",
    "        likelihood = (1-i)*pr0 + i*pr1\n",
    "        return -np.log(likelihood).sum()\n",
    "    \n",
    "    \n",
    "    def fit(self, start_params=None, maxiter=1000, maxfun=5000, **kwds):\n",
    "        if start_params == None:\n",
    "            start_params = [1,1]\n",
    "        return super(Rust, self).fit(start_params=start_params,\n",
    "                                       maxiter=maxiter, maxfun=maxfun, **kwds)\n",
    "    \n",
    "    \n",
    "model = Rust(data[:,0],data[:,1])\n",
    "\n",
    "result = model.fit()\n",
    "print(result.summary(xname=['theta_1', 'R']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7\n",
    "\n",
    "A) To accomodate $\\theta_{1A}$ and $\\theta_{2A}$ we would need to modify the dynamic program as follows\n",
    "\n",
    "$$V(a_t,i_t,\\epsilon_{jt}) = \\pi(a_t, i_t, \\epsilon_{it}) + max_{i_{t+1}} \\beta E(V(a_{t+1},i_{t+1},\\epsilon_{jt+1}) |a_t, \\epsilon_{jt}, i_t; \\theta_i)$$\n",
    "\n",
    "\n",
    "\n",
    "B) We would still have unobserved heterogeneity. However we could calculate the value of $\\epsilon_{it-1}$ and include it in the likelihood function\n",
    "\n",
    "$ L_{it} = Prob(i_{tj} | a_t, \\epsilon_{jt-1} )$\n",
    "\n",
    "C) If machines differ, then we can use fixed effects instead of making $\\epsilon_{it}$ a state variable.\n",
    "\n",
    "$L_{it} =  Prob(i_{tj} | a_t, \\alpha{j} )$. We could simulate the $\\alpha_j$'s\n",
    "\n",
    "D) One way to solve it would be simulating back to period 1\n",
    "\n",
    "E) Most there is not enough information to identify $\\lambda$ seperately from $\\alpha$\n",
    "\n",
    "### Part 8\n",
    "\n",
    "Below I estimate the model using the Hotz-Miller algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9611945001197668\n",
      "-2.275283957960293\n",
      "-2.956938566759297\n",
      "-3.4808951496253133\n",
      "-4.450349933958209\n"
     ]
    }
   ],
   "source": [
    "def hm_prob(a_max, theta1, cost, a_obs, i_obs):\n",
    "    \"\"\"estimated value function, based on the data\"\"\"\n",
    "    \n",
    "    #set up hotz miller probability est\n",
    "    kappa = np.zeros(a_max) \n",
    "    for a in range(1,a_max+1):\n",
    "        \n",
    "        #isolate decisions in this state\n",
    "        policy =  i_obs[np.where( a_obs == a)]\n",
    "        \n",
    "        #compute transition probabilities/state probabilites\n",
    "        pr_trans = policy.sum()/(1.* policy.shape[0])\n",
    "        pr_state =  policy.shape[0]/(1.*a_obs.shape[0])\n",
    "        \n",
    "        #calculate value function for this state\n",
    "        denom = (1 - (1-pr_state)*BETA*(1-pr_trans) - pr_trans*BETA*pr_state)\n",
    "        \n",
    "        numer = ( (1-pr_state)*(theta1*a  + GAMMA - np.log(1-pr_state)) + \n",
    "                 pr_state*(cost+ GAMMA - np.log(pr_state) ) )\n",
    "        \n",
    "        value = numer/denom\n",
    "        print value\n",
    "        \n",
    "        #calculate kappa using value function\n",
    "        kappa[a-1] = (np.exp(cost + BETA*pr_trans*value)/\n",
    "                     (np.exp(a*theta1 + BETA*(1-pr_trans)*value) + np.exp(cost + BETA*pr_trans*value)))\n",
    "        \n",
    "    return kappa\n",
    "    \n",
    "pr =  hm_prob(5, -1, -3, data[:,0], data[:,1])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "print np.array([1,0,1])[np.where([True,False,True])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
