{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Choice Probabilitiy Estimators in 5 Easy Steps!\n",
    "\n",
    "## Author: Eric Schulman\n",
    "\n",
    "The following guide demonstrates how to use conditional choice probability estimators in Python. It was written in part as a homework for the University of Texas second year course in industrial organization. These estimators are the normal way to think about the future's influences decisions in industrial organization and related fields.\n",
    "\n",
    "To demonstrate how to use (and implement) a CCP estimator, we recover parameters for the cost function in [Rust 1987](https://www.jstor.org/stable/1911259). Rust's paper considers the decision of a bus manager. The bus manager decides whether or not to replace bus engine for his fleet of buses in Madison, Wisconsion. Replacing the engine has a high cost in the present, but letting the engine accumulate mileage makes the bus more likely to break down in the future. Our goal is estimating parameters that tell us the importance of mileage when the bus manager decides to replace the engines.\n",
    "\n",
    "The bus manager's problem is very general and has become the 'mascot' for dynamic decisions in industrial organization. Many macroeconomics textbook excersizes call it a 'tree-cutting' problem. The bus engine replacement problem has The manager has a very simple 'yes' or 'no' decision. However, his 'yes' or 'no' depends on the future. To estimate the importance of mileage in the bus manager's decision, we must calculate a value function. A much simple method to predicting the bus manager's decision using mileage would be a logit. Why introduce a value function? Well, Rust wanted to predict agents behavior. Rust demonstrated that adding a structural 'assumption', that agent's discount factor $\\beta >0$, does a better job predicitng the probability of replacment. \n",
    "\n",
    "\n",
    "Rust's called his approach to predicting agent's choices in this situation the nested fixed point algorithm (NFXP). To think about the future, he wrote a Bellman equation that contained the expected value of 'yes' and 'no' based on the current mileage. He alternated between calculating the fixed point of the Bellman Operator and finding parameters that told him the value of the mileage and using a maximum likelihood routine. Rust has code for solving this value function and estimate parameters in Gauss on his [website](https://editorialexpress.com/jrust/nfxp.html). To find the fixed point of the Bellman operator in Python you can find code [here](https://nbviewer.jupyter.org/github/QuantEcon/QuantEcon.notebooks/blob/master/ddp_ex_rust96_py.ipynb).\n",
    "\n",
    "The more recent approach to predicting dynamic choices is called conditional choice probability (CCP) estimation. This approach works similar to Rust's assymptotically. However, it simplifies Rust's NFXP. Instead of embedding a value function into a MLE routine every time, you start with a simple estimate of the choice probabilities and adjust this estimate to account for the future. Using the choice probabilities, you can calculate an estimate of the value function. This value function will help you adjust your estimates of how mileage influences the replacement probability. This approach was first introduced to the literature in [Hotz Miller 1993](https://www.jstor.org/stable/2298122). The code and data I used for this guide from comes from Victor Aguirregabiria and Pedro Mira's website accompanying their paper [Aguirregabiria Mira 2002](http://individual.utoronto.ca/vaguirre/wpapers/program_code_survey_joe_2008.html) (more on them later).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Pre-processing the data and structural constants\n",
    "\n",
    "In this step 2 important things happen:\n",
    "\n",
    "1. We set the discount factor. The most important aspect of CCP is setting your discount factor. Implicitly, our choice is an assumption about the bus manager because nothing in the data tells us about agent's discount factor (for more about this see [Magnac Thesmar 2002](https://www.jstor.org/stable/2692293)). All we see are mileage and replacment decisions. This assumption better fits the data.\n",
    "\n",
    "2. We discretize our continous data on mileage so we can think about the bus manager's problem as a Markov Decision process to calculate our Bellman equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.interpolate import interp1d #pre written interpolation function\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from scipy import stats #for kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                              162\n",
      "group                        530875\n",
      "date            1985-04-01 00:00:00\n",
      "replace                           1\n",
      "miles                      0.388254\n",
      "replace_next                      1\n",
      "miles_next                 0.388254\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#fix the bus .dat from augirregabiria and Mira's website\n",
    "data = np.fromfile('bus1234.dat')\n",
    "data = data.reshape(len(data)/6,6)\n",
    "data = pd.DataFrame(data,columns=['id','group','year','month','replace','miles'])\n",
    "\n",
    "#save to .csv so other people don't need to be confused\n",
    "data.to_csv(\"bus1234.csv\")\n",
    "\n",
    "#divide by 1e6 (use the same scale are Rust and AM)\n",
    "data['miles'] = (data['miles'])/1e6\n",
    "\n",
    "#switch to date time for ease \n",
    "data['date'] = pd.to_datetime(data[['year', 'month']].assign(Day=1))\n",
    "data = data[['id','group','date','replace','miles']]\n",
    "\n",
    "#lag date\n",
    "date_lag = data.copy()\n",
    "date_lag['date'] = date_lag['date'] - pd.DateOffset(months=1)\n",
    "data = data.merge(date_lag, how='left', on=['id','group','date'] , suffixes=('','_next'))\n",
    "data = data.dropna()\n",
    "\n",
    "print data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "BETA = .9999\n",
    "GAMMA = .5772 #euler's constant\n",
    "\n",
    "#size of step in discretization\n",
    "STEP = .002\n",
    "\n",
    "#make states global variables\n",
    "STATES = np.arange(data['miles'].min(),data['miles'].max() + STEP, STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculating Choice Probabilities 'Non-Parametrically'\n",
    "\n",
    "To do CCP estimation, we must estimate probability of 'yes' and 'no' using very simple methods.\n",
    "\n",
    "This requires naively estimating:\n",
    "1. The amount that each bus's mileage $x$ will increase depending on the engine replacement decision $i$\n",
    "2. The probability of engine replacement decision $i$ and 'conditional on mileage $x$.\n",
    "\n",
    "I estimated these probabilities using the same approach as Aguirregabiria and Mira. However, you can experiment with other (consistent) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miles_pdf(i_obs, x_obs, x_next):\n",
    "    \"\"\"estimation of mileage pdf following AM using the\n",
    "    kernel function\n",
    "    \n",
    "    this corresponds to pdfdx in AM's code\"\"\"\n",
    "    \n",
    "    #figure out max number of steps\n",
    "    dx = (1-i_obs)*(x_next - x_obs) + i_obs*x_next\n",
    "    \n",
    "    #number of 'transition' states\n",
    "    dx_states = np.arange(dx.min(),dx.max() +STEP , STEP)\n",
    "    \n",
    "    #use kernel groups to make pdf\n",
    "    kernel1 = stats.gaussian_kde(dx, bw_method='silverman')\n",
    "    pdfdx = kernel1(dx_states)\n",
    "    \n",
    "    return np.array([pdfdx/pdfdx.sum()]).transpose()\n",
    "\n",
    "\n",
    "MILES_PDF = miles_pdf(data['replace'], data['miles'], data['miles_next'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_1(i_obs, x_obs , x_next):\n",
    "    \"\"\"calculate transitions probabilities,\n",
    "    non-parametrically\n",
    "    \n",
    "    this corresponds to fmat2 in AM's code\"\"\"\n",
    "    \n",
    "    #transitions when i=1\n",
    "    pdfdx = miles_pdf(i_obs, x_obs, x_next).transpose()\n",
    "    \n",
    "    #zero probability of transitioning to large states\n",
    "    zeros = np.zeros( (len(STATES),len(STATES)-pdfdx.shape[1]) )\n",
    "    \n",
    "    #transitioning to first state and 'jumping' dx states\n",
    "    fmat1 = np.tile(pdfdx,(len(STATES),1))\n",
    "    fmat1 = np.concatenate( (fmat1, zeros), axis=1 )\n",
    "\n",
    "    return fmat1\n",
    "\n",
    "FMAT1 = transition_1(data['replace'], data['miles'],data['miles_next'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_0(i_obs, x_obs, x_next):\n",
    "    \"\"\"calculate transitions probabilities,\n",
    "    non-parametrically\n",
    "    \n",
    "    this corresponds to fmat1 in AM's code\"\"\"\n",
    "    \n",
    "    pdfdx = miles_pdf(i_obs, x_obs, x_next).transpose()\n",
    "    \n",
    "    #initialize fmat array, transitions when i=0\n",
    "    end_zeros = np.zeros((1, len(STATES) - pdfdx.shape[1]))\n",
    "    fmat0 = np.concatenate( (pdfdx, end_zeros), axis=1 )\n",
    "\n",
    "    for row in range(1, len(STATES)):\n",
    "        \n",
    "        #this corresponds to colz i think\n",
    "        cutoff = ( len(STATES) - row - pdfdx.shape[1] )\n",
    "        \n",
    "        #case 1 far enough from the 'end' of the matrix\n",
    "        if cutoff >= 0:\n",
    "            start_zeros = np.zeros((1,row))\n",
    "            end_zeros = np.zeros((1, len(STATES) - pdfdx.shape[1] - row))\n",
    "            fmat_new = np.concatenate( (start_zeros, pdfdx, end_zeros), axis=1 )\n",
    "            fmat0 = np.concatenate((fmat0, fmat_new))\n",
    "       \n",
    "        #case 2, too far from the end and need to adjust probs\n",
    "        else:\n",
    "            pdf_adj = pdfdx[:,0:cutoff]\n",
    "            pdf_adj = pdf_adj/pdf_adj.sum(axis=1)\n",
    "            \n",
    "            start_zeros = np.zeros((1,row))\n",
    "            fmat_new = np.concatenate( (start_zeros, pdf_adj), axis=1 )\n",
    "            fmat0 = np.concatenate((fmat0, fmat_new))\n",
    "            \n",
    "    return fmat0\n",
    "\n",
    "FMAT0 = transition_0(data['replace'],data['miles'],data['miles_next'])\n",
    "\n",
    "PR_TRANS = FMAT0, FMAT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036201\n",
      "         Iterations 23\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   No. Observations:                 8156\n",
      "Model:                          Logit   Df Residuals:                     8152\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Tue, 15 Jan 2019   Pseudo R-squ.:                  0.1671\n",
      "Time:                        14:20:03   Log-Likelihood:                -295.26\n",
      "converged:                       True   LL-Null:                       -354.51\n",
      "                                        LLR p-value:                 1.623e-25\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -17.3136      4.188     -4.134      0.000     -25.522      -9.105\n",
      "x1           149.3089     56.675      2.634      0.008      38.228     260.390\n",
      "x2          -553.1128    245.812     -2.250      0.024   -1034.895     -71.330\n",
      "x3           697.5936    340.855      2.047      0.041      29.531    1365.657\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.37 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "def initial_pr(i_obs, x_obs, d=0):\n",
    "    \"\"\"initial the probability of view a given state following AM.\n",
    "    just involves logit to predict\n",
    "    \n",
    "    Third arguement involves display\"\"\"\n",
    "    \n",
    "    X = np.array([x_obs, x_obs**2, x_obs**3]).transpose()\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.Logit(i_obs,X)\n",
    "    fit = model.fit(disp=d)\n",
    "    if d: print fit.summary()\n",
    "    \n",
    "    x_states = np.array([STATES, STATES**2, STATES**3]).transpose()\n",
    "    x_states = sm.add_constant(x_states)\n",
    "    \n",
    "    return fit.predict(x_states)\n",
    "\n",
    "PR_OBS = initial_pr(data['replace'], data['miles'], d=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Alternative Value Function Representation\n",
    "\n",
    "Now, that we have non-parametric estimates of the transition matrix and and choice probabilities we can calculate the value function to learn the importance of mileage to the bus manager.\n",
    "\n",
    "First let's define some variables:\n",
    "\n",
    "* Let $F(i)$ be the transition matrix between states depending on the replacement decision $i$. We learned this above using the Guassian kernel.\n",
    "\n",
    "* Let $P$ be the probability of replacing the engine conditioanl on the mileage. We learned this above using the logit.\n",
    "\n",
    "* Let $u(i,x;\\theta)$ be the cost function.\n",
    "\n",
    "* Implicitly, there is uncertainity $\\epsilon$ each period. We give this the parametric extereme value distribution.\n",
    "\n",
    "Using just these objects, the model parameters (which we are trying to learn), we can write the value function below\n",
    "\n",
    "$$V = [I_m - \\beta[(1-P) \\otimes F(0) + P \\otimes F(1)] ]^{-1} [(1-P)*(u(0,x;\\theta) + \\gamma -ln(1-P)) + P*( u(1,x;\\theta) + \\gamma -ln(P) ) ]$$\n",
    "\n",
    "This corresponds to equation (8) in Aguirregabiria Mira 2002. You can find a formal derivation of this in Hotz Miller 1993 and Aguirregabira Mira's paper.\n",
    "\n",
    "For the purposes of clarifying some symbols:\n",
    "\n",
    "* $\\otimes$ is the Kroenecker product (i.e. column wise). I implemented this by tiling the vector.\n",
    "* And, $*$ is the Hadamard produce (i.e. element wise)\n",
    "\n",
    "\n",
    "Also note, I parameterized the cost function using a python `lambda` expression. For those unfamilar, I am passing another function into the routine for calculating the value funciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hm_value(params, cost, pr_obs, pr_trans):\n",
    "    \"\"\"calculate value function using hotz miller approach\"\"\"\n",
    "    \n",
    "    #set up matrices, transition is deterministic\n",
    "    trans0, trans1 = pr_trans\n",
    "    \n",
    "    #calculate value function for all state\n",
    "    pr_tile = np.tile( pr_obs.reshape( len(STATES) ,1), (1, len(STATES) ))\n",
    "    \n",
    "    denom = (np.identity( len(STATES) ) - BETA*(1-pr_tile)*trans0 - BETA*pr_tile*trans1)\n",
    "    \n",
    "    numer = ( (1-pr_obs)*(cost(params, STATES, 0) + GAMMA - np.log(1-pr_obs)) + \n",
    "                 pr_obs*(cost(params, STATES, 1) + GAMMA - np.log(pr_obs) ) )\n",
    "    \n",
    "    value = np.linalg.inv(denom).dot(numer)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: (Psuedo) Maximum Likelihood Estimaton\n",
    "\n",
    "With the value function we just calculated, we can learn a 'psuedo' likehood of an observation. If we think agents have discount factor $\\beta >0$ the true likelihood of each observation would involve calculating the value function each time we calculate the likelihood. Rust actually calculates this likelihood each time using value function iteration.\n",
    "\n",
    "You might be wondering why recalculate the choice probabilities? Well, now the choice probabilities depend on the parameters we wanted to learn. Ultimately, we wanted to learn how much mileage influences our decision for the future. \n",
    "\n",
    "Even though we have the probability of replacement from the data. We recalculate it using the value function. Instead of recalculating the value function on each call to the likelihood function we are just adusting our initial estimates of the likelihood. It turns out that these approaches are both assymptoically consistent.\n",
    "\n",
    "\n",
    "The formula for the likelihod based on the value function is\n",
    "\n",
    "\n",
    "This is very similiar to the logit we used to estimate the choice probabilities before. Now, the mileage is influenced by our value function.\n",
    "\n",
    " $\\Psi$ in Aguirregabiria Mira 2002. \n",
    "\n",
    "Aguirregabiria Mira parametrize $\\Psi$ using the extreme value distribution below Proposition 3 in their paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hm_prob(params, cost, pr_obs, pr_trans):\n",
    "    \"\"\"calculate kappa (i.e. CCP likelihood) using value function\"\"\"\n",
    "    \n",
    "    value = hm_value(params, cost, pr_obs, pr_trans)\n",
    "    value = value - value.min() #subtract out smallest value\n",
    "    trans0, trans1 = pr_trans\n",
    "    \n",
    "    delta1 = np.exp( cost(params, STATES, 1) + BETA*trans1.dot(value))\n",
    "    delta0 = np.exp( cost(params, STATES, 0) + BETA*trans0.dot(value) )\n",
    "    \n",
    "    return delta1/(delta1+delta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCP(GenericLikelihoodModel):\n",
    "    \"\"\"class for estimating the values of R and theta\n",
    "    using the CCP routine and the helper functions\n",
    "    above\"\"\"\n",
    "    \n",
    "    def __init__(self, i, x, x_next, params, cost, **kwds):\n",
    "        \"\"\"initialize the class\n",
    "        \n",
    "        i - replacement decisions\n",
    "        x - miles\n",
    "        x_next - next periods miles\n",
    "        params - names for cost function parameters\n",
    "        cost - cost function specification, takes agruements (params, x, i) \"\"\"\n",
    "        \n",
    "        super(CCP, self).__init__(i, x, **kwds)\n",
    "        \n",
    "        #data\n",
    "        self.endog = i #these names don't work exactly\n",
    "        self.exog = x #the idea is that x is mean indep of epsilon\n",
    "        self.x_next = x_next\n",
    "        \n",
    "        #transitions\n",
    "        self.pr_obs = initial_pr(i, x)\n",
    "        self.trans =  transition_0(i,x,x_next), transition_1(i,x,x_next)\n",
    "        \n",
    "        #should probably make these class parameters\n",
    "        self.num_states = ( x.max()/STEP).astype(int) + 2\n",
    "        self.states = np.arange(x.min(),x.max() + STEP, STEP)\n",
    "        \n",
    "        #initial model fit\n",
    "        self.cost = cost\n",
    "        self.num_params = len(params)\n",
    "        self.data.xnames =  params\n",
    "        self.results = self.fit( start_params=np.ones(self.num_params) )\n",
    "        \n",
    "        \n",
    "    def nloglikeobs(self, params, v=False):\n",
    "        \"\"\"psuedo log likelihood function for the CCP estimator\"\"\"\n",
    "        \n",
    "        # Input our data into the model\n",
    "        i = self.endog\n",
    "        x = (self.exog/STEP).astype(int)*STEP #discretized x\n",
    "           \n",
    "        #set up hm state pr\n",
    "        prob = hm_prob(params, self.cost, self.pr_obs, self.trans).transpose()\n",
    "        prob = interp1d(self.states, prob)\n",
    "        prob = prob(x)\n",
    "        \n",
    "        log_likelihood = (1-i)*np.log(1-prob) + i*np.log(prob)\n",
    "        \n",
    "        return -log_likelihood.sum()\n",
    "    \n",
    "    \n",
    "    def iterate(self, numiter):\n",
    "        \"\"\"iterate the Hotz Miller estimation procedure 'numiter' times\"\"\"\n",
    "        i = 0\n",
    "        while(i < numiter):\n",
    "            #update pr_obs based on parameters\n",
    "            self.pr_obs = hm_prob(self.results.params, self.cost, self.pr_obs, self.trans)\n",
    "            \n",
    "            #refit the model\n",
    "            self.results = self.fit(start_params=np.ones(self.num_params))\n",
    "            i = i +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036544\n",
      "         Iterations: 63\n",
      "         Function evaluations: 120\n",
      "                                 CCP Results                                  \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   Log-Likelihood:                -298.05\n",
      "Model:                            CCP   AIC:                             598.1\n",
      "Method:            Maximum Likelihood   BIC:                             605.1\n",
      "Date:                Tue, 15 Jan 2019                                         \n",
      "Time:                        14:37:02                                         \n",
      "No. Observations:                8156                                         \n",
      "Df Residuals:                    8155                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta1        -0.5218      0.093     -5.632      0.000      -0.703      -0.340\n",
      "theta2       -10.1309      0.983    -10.301      0.000     -12.058      -8.203\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "#define cost functon using lambda expression\n",
    "LINEAR_COST = lambda params, x, i: (1-i)*x*params[i] + i*params[i]\n",
    "\n",
    "model_ccp = CCP(data['replace'], data['miles'], data['miles_next'], ['theta1','theta2'], LINEAR_COST)\n",
    "print model_ccp.results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Costs\n",
    "\n",
    "We can see that the change in specification does not drastically change the estimates. Considering the limited data, the cost function is probability not identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036261\n",
      "         Iterations: 147\n",
      "         Function evaluations: 260\n",
      "                                 CCP Results                                  \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   Log-Likelihood:                -295.75\n",
      "Model:                            CCP   AIC:                             593.5\n",
      "Method:            Maximum Likelihood   BIC:                             600.5\n",
      "Date:                Tue, 15 Jan 2019                                         \n",
      "Time:                        14:35:24                                         \n",
      "No. Observations:                8156                                         \n",
      "Df Residuals:                    8155                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta1        -2.5281      1.088     -2.324      0.020      -4.660      -0.396\n",
      "theta2         4.0832      2.156      1.894      0.058      -0.142       8.308\n",
      "theta3       -15.2180      3.255     -4.675      0.000     -21.598      -8.838\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "QUAD_COST = lambda params, x, i: (1-i)*(x*params[0] + x**2*params[1]) + i*params[2]\n",
    "\n",
    "model_ccp = CCP(data['replace'], data['miles'], data['miles_next'], ['theta1','theta2', 'theta3'], QUAD_COST)\n",
    "print model_ccp.results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Iterating the Model\n",
    "\n",
    "It turns out that you can iterate upon these estimates to converge to the true policy function in Rust 1987. I iterate the value function below. For more information on this you can see.\n",
    "\n",
    "Victor Aguirregabiria and Pedro Mira's website:\n",
    "http://individual.utoronto.ca/vaguirre/wpapers/program_code_survey_joe_2008.html\n",
    "\n",
    "Victor Aguirregabiria and Pedro Mira's 2002 paper\n",
    "https://www.jstor.org/stable/3082006\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036544\n",
      "         Iterations: 63\n",
      "         Function evaluations: 120\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036530\n",
      "         Iterations: 62\n",
      "         Function evaluations: 117\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036528\n",
      "         Iterations: 63\n",
      "         Function evaluations: 118\n",
      "                                 CCP Results                                  \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   Log-Likelihood:                -297.93\n",
      "Model:                            CCP   AIC:                             597.9\n",
      "Method:            Maximum Likelihood   BIC:                             604.9\n",
      "Date:                Tue, 15 Jan 2019                                         \n",
      "Time:                        14:35:56                                         \n",
      "No. Observations:                8156                                         \n",
      "Df Residuals:                    8155                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta1        -0.5411      0.091     -5.978      0.000      -0.719      -0.364\n",
      "theta2       -10.2082      0.955    -10.692      0.000     -12.080      -8.337\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "model_ccp = CCP(data['replace'], data['miles'], data['miles_next'], ['theta1','theta2'], LINEAR_COST)\n",
    "model_ccp.iterate(2)\n",
    "print model_ccp.results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
