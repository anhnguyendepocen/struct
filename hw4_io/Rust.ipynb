{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Choice Probabilitiy Estimators in 4 Easy Steps!\n",
    "\n",
    "## Author: Eric Schulman\n",
    "\n",
    "The following guide demonstrates how to use a conditional choice probability estimators in Python. It was written in part as a homework for the University of Texas second year course in industrial organization. These estimators have become the normal way to think about how various factors influence decisions that depend on the future in industrial organization and related fields.\n",
    "\n",
    "To demonstrate how to use an implement a CCP estimator, we recover parameters for the cost function in Rust 1987. Rust's paper considers the decision of a bus manager. The bus manager had to decide whether or not to replace a bus engine for his fleet of buses in Madison Wisconsion. This is a very simple 'yes' or 'no' decision. However 'yes' or 'no' depend on the future. The goal is to recover parameters that tell us the importance of mileage when the bus manager decides to replace the engines. \n",
    "\n",
    "This decision problem is really general. You can think of it as a 'tree cutting' problem. Agents do not only think about the present when making this decision. In order to think about the future, you must consider a Bellman equation that contains the expected value of 'yes' and 'no'. This function is called the value funciton. Rust's approach involved calculating a value function explicitly. He alternated between picking paramters using MLE and then estimating parameters in the value function. \n",
    "\n",
    "John Rust's website:\n",
    "https://editorialexpress.com/jrust/nfxp.html\n",
    "\n",
    "\n",
    "CCP estimators differ from Rust's original approach because of their focus on prediciton. Instead of looking for a value function and estimating its likelihood to find parameters, you solve for the parameters using the choice probailities in the data. Once you have this value function, you can recalculate what the the choice probabilities, would be if you varied the model paramters. More over, you can maximize the likelihood of the 'conditional' choice probabilities to find consistent estimates of the parameters in the model.\n",
    "\n",
    "This approach was first discovered formally in Joseph Hotz and Bob Miller's paper\n",
    "https://www.jstor.org/stable/2298122\n",
    "\n",
    "However, the code and data I modeled this guide from comes from Victor Aguirregabiria and Pedro Mira's website (more on them later):\n",
    "http://individual.utoronto.ca/vaguirre/wpapers/program_code_survey_joe_2008.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.interpolate import interp1d #pre written interpolation function\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from scipy import stats #for kernel function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Pre-processing the data and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                              162\n",
      "group                        530875\n",
      "date            1985-04-01 00:00:00\n",
      "replace                           1\n",
      "miles                      0.388254\n",
      "replace_next                      1\n",
      "miles_next                 0.388254\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#fix the bus .dat from augirregabiria and Mira's website\n",
    "data = np.fromfile('bus1234.dat')\n",
    "data = data.reshape(len(data)/6,6)\n",
    "data = pd.DataFrame(data,columns=['id','group','year','month','replace','miles'])\n",
    "\n",
    "#save to .csv so other people don't need to be confused\n",
    "data.to_csv(\"bus1234.csv\")\n",
    "\n",
    "#divide by 1e6 (use the same scale are Rust and AM)\n",
    "data['miles'] = (data['miles'])/1e6\n",
    "\n",
    "#switch to date time for ease \n",
    "data['date'] = pd.to_datetime(data[['year', 'month']].assign(Day=1))\n",
    "data = data[['id','group','date','replace','miles']]\n",
    "\n",
    "#lag date\n",
    "date_lag = data.copy()\n",
    "date_lag['date'] = date_lag['date'] - pd.DateOffset(months=1)\n",
    "data = data.merge(date_lag, how='left', on=['id','group','date'] , suffixes=('','_next'))\n",
    "data = data.dropna()\n",
    "\n",
    "print data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "BETA = .9999\n",
    "GAMMA = .5772 #euler's constant\n",
    "\n",
    "#size of step in discretization\n",
    "STEP = .002\n",
    "\n",
    "#make states global variables\n",
    "STATES = np.arange(data['miles'].min(),data['miles'].max() + STEP, STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculating Choice Probabilities 'Non-Parametrically'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miles_pdf(i_obs, x_obs, x_next):\n",
    "    \"\"\"estimation of mileage pdf following AM using the\n",
    "    kernel function\n",
    "    \n",
    "    this corresponds to pdfdx in AM's code\"\"\"\n",
    "    \n",
    "    #figure out max number of steps\n",
    "    dx = (1-i_obs)*(x_next - x_obs) + i_obs*x_next\n",
    "    \n",
    "    #number of 'transition' states\n",
    "    dx_states = np.arange(dx.min(),dx.max() +STEP , STEP)\n",
    "    \n",
    "    #use kernel groups to make pdf\n",
    "    kernel1 = stats.gaussian_kde(dx, bw_method='silverman')\n",
    "    pdfdx = kernel1(dx_states)\n",
    "    \n",
    "    return np.array([pdfdx/pdfdx.sum()]).transpose()\n",
    "\n",
    "\n",
    "MILES_PDF = miles_pdf(data['replace'], data['miles'], data['miles_next'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_1(i_obs, x_obs , x_next):\n",
    "    \"\"\"calculate transitions probabilities,\n",
    "    non-parametrically\n",
    "    \n",
    "    this corresponds to fmat2 in AM's code\"\"\"\n",
    "    \n",
    "    #transitions when i=1\n",
    "    pdfdx = miles_pdf(i_obs, x_obs, x_next).transpose()\n",
    "    \n",
    "    #zero probability of transitioning to large states\n",
    "    zeros = np.zeros( (len(STATES),len(STATES)-pdfdx.shape[1]) )\n",
    "    \n",
    "    #transitioning to first state and 'jumping' dx states\n",
    "    fmat1 = np.tile(pdfdx,(len(STATES),1))\n",
    "    fmat1 = np.concatenate( (fmat1, zeros), axis=1 )\n",
    "\n",
    "    return fmat1\n",
    "\n",
    "FMAT1 = transition_1(data['replace'], data['miles'],data['miles_next'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_0(i_obs, x_obs , x_next):\n",
    "    \"\"\"calculate transitions probabilities,\n",
    "    non-parametrically\n",
    "    \n",
    "    this corresponds to fmat1 in AM's code\"\"\"\n",
    "    \n",
    "    pdfdx = miles_pdf(i_obs, x_obs, x_next).transpose()\n",
    "    \n",
    "    #initialize fmat array, transitions when i=0\n",
    "    end_zeros = np.zeros((1, len(STATES) - pdfdx.shape[1]))\n",
    "    fmat0 = np.concatenate( (pdfdx, end_zeros), axis=1 )\n",
    "\n",
    "    for row in range(1, len(STATES)):\n",
    "        \n",
    "        #this corresponds to colz i think\n",
    "        cutoff = ( len(STATES) - row - pdfdx.shape[1] )\n",
    "        \n",
    "        #case 1 far enough from the 'end' of the matrix\n",
    "        if cutoff >= 0:\n",
    "            start_zeros = np.zeros((1,row))\n",
    "            end_zeros = np.zeros((1, len(STATES) - pdfdx.shape[1] - row))\n",
    "            fmat_new = np.concatenate( (start_zeros, pdfdx, end_zeros), axis=1 )\n",
    "            fmat0 = np.concatenate((fmat0, fmat_new))\n",
    "       \n",
    "        #case 2, too far from the end and need to adjust probs\n",
    "        else:\n",
    "            pdf_adj = pdfdx[:,0:cutoff]\n",
    "            pdf_adj = pdf_adj/pdf_adj.sum(axis=1)\n",
    "            \n",
    "            start_zeros = np.zeros((1,row))\n",
    "            fmat_new = np.concatenate( (start_zeros, pdf_adj), axis=1 )\n",
    "            fmat0 = np.concatenate((fmat0, fmat_new))\n",
    "            \n",
    "    return fmat0\n",
    "\n",
    "FMAT0 = transition_0(data['replace'],data['miles'],data['miles_next'])\n",
    "\n",
    "PR_TRANS = FMAT0, FMAT1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036201\n",
      "         Iterations 23\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   No. Observations:                 8156\n",
      "Model:                          Logit   Df Residuals:                     8152\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Tue, 15 Jan 2019   Pseudo R-squ.:                  0.1671\n",
      "Time:                        14:20:03   Log-Likelihood:                -295.26\n",
      "converged:                       True   LL-Null:                       -354.51\n",
      "                                        LLR p-value:                 1.623e-25\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -17.3136      4.188     -4.134      0.000     -25.522      -9.105\n",
      "x1           149.3089     56.675      2.634      0.008      38.228     260.390\n",
      "x2          -553.1128    245.812     -2.250      0.024   -1034.895     -71.330\n",
      "x3           697.5936    340.855      2.047      0.041      29.531    1365.657\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.37 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "def initial_pr(i_obs, x_obs, d=0):\n",
    "    \"\"\"initial the probability of view a given state following AM.\n",
    "    Seems like it just involves logit to predict\n",
    "    \n",
    "    Third arguement involves display\"\"\"\n",
    "    \n",
    "    X = np.array([x_obs, x_obs**2, x_obs**3]).transpose()\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.Logit(i_obs,X)\n",
    "    fit = model.fit(disp=d)\n",
    "    if d: print fit.summary()\n",
    "    \n",
    "    x_states = np.array([STATES, STATES**2, STATES**3]).transpose()\n",
    "    x_states = sm.add_constant(x_states)\n",
    "    \n",
    "    return fit.predict(x_states)\n",
    "\n",
    "PR_OBS = initial_pr(data['replace'], data['miles'], d=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Alternative Value Function Representation\n",
    "\n",
    "To do CCP estimation, you figure out the probability of 'Yes' and 'No' in the data without considering the process that agents use to make their decision. To recover the parameters involved with agents decisions you relate these parameters to the probability of 'yes' and 'no' in the data. Formally, this invovles solving for the value function in terms of the probability of replacment and the parameters.\n",
    "\n",
    "You can find a formal derivation of this in Hotz Miller (1993) and Aguirregabira Mira (2002). However the two key relationships are states below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hm_value(params, cost, pr_obs, pr_trans):\n",
    "    \"\"\"calculate value function using hotz miller approach\"\"\"\n",
    "    \n",
    "    #set up matrices, transition is deterministic\n",
    "    trans0, trans1 = pr_trans\n",
    "    \n",
    "    #calculate value function for all state\n",
    "    pr_tile = np.tile( pr_obs.reshape( len(STATES) ,1), (1, len(STATES) ))\n",
    "    \n",
    "    denom = (np.identity( len(STATES) ) - BETA*(1-pr_tile)*trans0 - BETA*pr_tile*trans1)\n",
    "    \n",
    "    numer = ( (1-pr_obs)*(cost(params, STATES, 0) + GAMMA - np.log(1-pr_obs)) + \n",
    "                 pr_obs*(cost(params, STATES, 1) + GAMMA - np.log(pr_obs) ) )\n",
    "    \n",
    "    value = np.linalg.inv(denom).dot(numer)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hm_prob(params, cost, pr_obs, pr_trans):\n",
    "    \"\"\"calculate kappa (i.e. CCP likelihood) using value function\"\"\"\n",
    "    \n",
    "    value = hm_value(params, cost, pr_obs, pr_trans)\n",
    "    value = value - value.min() #subtract out smallest value\n",
    "    trans0, trans1 = pr_trans\n",
    "    \n",
    "    delta1 = np.exp( cost(params, STATES, 1) + BETA*trans1.dot(value))\n",
    "    delta0 = np.exp( cost(params, STATES, 0) + BETA*trans0.dot(value) )\n",
    "    \n",
    "    return delta1/(delta1+delta0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: (Psuedo) Maximum Likelihood Estimaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCP(GenericLikelihoodModel):\n",
    "    \"\"\"class for estimating the values of R and theta\n",
    "    using the CCP routine and the helper functions\n",
    "    above\"\"\"\n",
    "    \n",
    "    def __init__(self, i, x, x_next, params, cost, **kwds):\n",
    "        \"\"\"initialize the class\n",
    "        \n",
    "        i - replacement decisions\n",
    "        x - miles\n",
    "        x_next - next periods miles\n",
    "        params - names for cost function parameters\n",
    "        cost - cost function specification, takes agruements (params, x, i) \"\"\"\n",
    "        \n",
    "        super(CCP, self).__init__(i, x, **kwds)\n",
    "        \n",
    "        #data\n",
    "        self.endog = i #these names don't work exactly\n",
    "        self.exog = x #the idea is that x is mean indep of epsilon\n",
    "        self.x_next = x_next\n",
    "        \n",
    "        #transitions\n",
    "        self.pr_obs = initial_pr(i, x)\n",
    "        self.trans =  transition_0(i,x,x_next), transition_1(i,x,x_next)\n",
    "        \n",
    "        #should probably make these class parameters\n",
    "        self.num_states = ( x.max()/STEP).astype(int) + 2\n",
    "        self.states = np.arange(x.min(),x.max() + STEP, STEP)\n",
    "        \n",
    "        #initial model fit\n",
    "        self.cost = cost\n",
    "        self.num_params = len(params)\n",
    "        self.data.xnames =  params\n",
    "        self.results = self.fit( start_params=np.ones(self.num_params) )\n",
    "        \n",
    "        \n",
    "    def nloglikeobs(self, params, v=False):\n",
    "        \"\"\"psuedo log likelihood function for the CCP estimator\"\"\"\n",
    "        \n",
    "        # Input our data into the model\n",
    "        i = self.endog\n",
    "        x = (self.exog/STEP).astype(int)*STEP #discretized x\n",
    "           \n",
    "        #set up hm state pr\n",
    "        prob = hm_prob(params, self.cost, self.pr_obs, self.trans).transpose()\n",
    "        prob = interp1d(self.states, prob)\n",
    "        prob = prob(x)\n",
    "        \n",
    "        log_likelihood = (1-i)*np.log(1-prob) + i*np.log(prob)\n",
    "        \n",
    "        return -log_likelihood.sum()\n",
    "    \n",
    "    \n",
    "    def iterate(self, numiter):\n",
    "        \"\"\"iterate the Hotz Miller estimation procedure 'numiter' times\"\"\"\n",
    "        i = 0\n",
    "        while(i < numiter):\n",
    "            #update pr_obs based on parameters\n",
    "            self.pr_obs = hm_prob(self.results.params, self.cost, self.pr_obs, self.trans)\n",
    "            \n",
    "            #refit the model\n",
    "            self.results = self.fit(start_params=np.ones(self.num_params))\n",
    "            i = i +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036544\n",
      "         Iterations: 63\n",
      "         Function evaluations: 120\n",
      "                                 CCP Results                                  \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   Log-Likelihood:                -298.05\n",
      "Model:                            CCP   AIC:                             598.1\n",
      "Method:            Maximum Likelihood   BIC:                             605.1\n",
      "Date:                Tue, 15 Jan 2019                                         \n",
      "Time:                        14:37:02                                         \n",
      "No. Observations:                8156                                         \n",
      "Df Residuals:                    8155                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta1        -0.5218      0.093     -5.632      0.000      -0.703      -0.340\n",
      "theta2       -10.1309      0.983    -10.301      0.000     -12.058      -8.203\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "#define cost functon using lambda expression\n",
    "LINEAR_COST = lambda params, x, i: (1-i)*x*params[i] + i*params[i]\n",
    "\n",
    "model_ccp = CCP(data['replace'], data['miles'], data['miles_next'], ['theta1','theta2'], LINEAR_COST)\n",
    "print model_ccp.results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Costs\n",
    "\n",
    "We can see that the change in specification does not drastically change the estimates. Considering the limited data, the cost function is probability not identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036261\n",
      "         Iterations: 147\n",
      "         Function evaluations: 260\n",
      "                                 CCP Results                                  \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   Log-Likelihood:                -295.75\n",
      "Model:                            CCP   AIC:                             593.5\n",
      "Method:            Maximum Likelihood   BIC:                             600.5\n",
      "Date:                Tue, 15 Jan 2019                                         \n",
      "Time:                        14:35:24                                         \n",
      "No. Observations:                8156                                         \n",
      "Df Residuals:                    8155                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta1        -2.5281      1.088     -2.324      0.020      -4.660      -0.396\n",
      "theta2         4.0832      2.156      1.894      0.058      -0.142       8.308\n",
      "theta3       -15.2180      3.255     -4.675      0.000     -21.598      -8.838\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "QUAD_COST = lambda params, x, i: (1-i)*(x*params[0] + x**2*params[1]) + i*params[2]\n",
    "\n",
    "model_ccp = CCP(data['replace'], data['miles'], data['miles_next'], ['theta1','theta2', 'theta3'], QUAD_COST)\n",
    "print model_ccp.results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Iterating the Model\n",
    "\n",
    "It turns out that you can iterate upon these estimates to converge to the true policy function in Rust 1987. I iterate the value function below. For more information on this you can see.\n",
    "\n",
    "Victor Aguirregabiria and Pedro Mira's website:\n",
    "http://individual.utoronto.ca/vaguirre/wpapers/program_code_survey_joe_2008.html\n",
    "\n",
    "Victor Aguirregabiria and Pedro Mira's 2002 paper\n",
    "https://www.jstor.org/stable/3082006\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036544\n",
      "         Iterations: 63\n",
      "         Function evaluations: 120\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036530\n",
      "         Iterations: 62\n",
      "         Function evaluations: 117\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036528\n",
      "         Iterations: 63\n",
      "         Function evaluations: 118\n",
      "                                 CCP Results                                  \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   Log-Likelihood:                -297.93\n",
      "Model:                            CCP   AIC:                             597.9\n",
      "Method:            Maximum Likelihood   BIC:                             604.9\n",
      "Date:                Tue, 15 Jan 2019                                         \n",
      "Time:                        14:35:56                                         \n",
      "No. Observations:                8156                                         \n",
      "Df Residuals:                    8155                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta1        -0.5411      0.091     -5.978      0.000      -0.719      -0.364\n",
      "theta2       -10.2082      0.955    -10.692      0.000     -12.080      -8.337\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "model_ccp = CCP(data['replace'], data['miles'], data['miles_next'], ['theta1','theta2'], LINEAR_COST)\n",
    "model_ccp.iterate(2)\n",
    "print model_ccp.results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
