{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Choice Probabilitiy Estimators in 5 Easy Steps!\n",
    "\n",
    "## Author: Eric Schulman\n",
    "\n",
    "The following guide demonstrates how to use conditional choice probability estimators in Python. It was written in part as a homework for the University of Texas second year course in industrial organization. These estimators are the normal way to think about how future influences decisions in industrial organization and related fields.\n",
    "\n",
    "To demonstrate how to use (and implement) a CCP estimator, we recover parameters for the cost function in [Rust 1987](https://www.jstor.org/stable/1911259). Rust's paper considers the decision of a bus manager. The bus manager decides whether or not to replace bus engines for his fleet of buses in Madison, Wisconsion. Replacing the engine has a high cost in the present but letting the engine accumulate mileage makes the bus more likely to break down in the future. Our goal is estimating parameters that tell us the importance of mileage when the bus manager decides to replace the engines. \n",
    "\n",
    "The bus manager's problem is very general and has become the 'mascot' for dynamic decisions in industrial organization. Many macroeconomics textbook excersizes call it a 'tree-cutting' problem. The bus manager has a very simple 'yes' or 'no' decision. However, his 'yes' or 'no' depends on the future. To estimate the importance of mileage in the bus manager's decision, we must calculate a value function. You could use a logit to predict the bus manager's decisions. However, Rust found that a model where agents considered the future predicted bus engine replacement decisions more accurately.\n",
    "\n",
    "Rust's called his approach to predicting agent's choices the nested fixed point algorithm (NFXP). To think about the future, he used the Bellman equation to calculate the expected value of 'yes' and 'no' based on the current mileage. Rust included this value function inside a logit. His estimation routine alternates between finding the Bellman operator's fixed point and estimating the logit with a maximum likelihood routine. Rust has code for solving this value function and estimate parameters in Gauss on his [website](https://editorialexpress.com/jrust/nfxp.html). To find the fixed point of the Bellman operator in Python you can find code [here](https://nbviewer.jupyter.org/github/QuantEcon/QuantEcon.notebooks/blob/master/ddp_ex_rust96_py.ipynb).\n",
    "\n",
    "The more recent approach to predicting dynamic choices is called conditional choice probability (CCP) estimation. This approach works similar to Rust's assymptotically. However, it simplifies Rust's NFXP. Instead of embedding a value function into a MLE routine, you start with a simple estimate of the choice probabilities and adjust this estimate to account for the future. Any estimate of the choice probabilities, will provide an estimate of the value function. This estimated value function will help you adjust your estimates of how mileage influences the replacement probability. This approach was first introduced to the literature in [Hotz Miller 1993](https://www.jstor.org/stable/2298122). The code and data I used for this guide from comes from Victor Aguirregabiria and Pedro Mira's [website](http://individual.utoronto.ca/vaguirre/wpapers/program_code_survey_joe_2008.html) accompanying their paper [Aguirregabiria Mira 2002](http://individual.utoronto.ca/vaguirre/wpapers/program_code_survey_joe_2008.html) (more on them later).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Pre-processing the data and structural constants\n",
    "\n",
    "There are two important factors involved with setting up the data\n",
    "1. First, we  must pick a discount factor. This is the most important aspect of the CCP estimator and distinguishes it from a logit. Implicitly, our choice is an assumption about the bus manager because nothing in the data tells us about agent's discount factor (for more about this see [Magnac Thesmar 2002](https://www.jstor.org/stable/2692293)). All we see are mileage and replacment decisions. Making this assumption better explains the data.\n",
    "\n",
    "2. Calculating the value function involves framing the bus manager's problem as a Markov Decision process. As a result, we discretize our continous data on mileage to make it easier to caclulate the value function for a given amount of mileage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.interpolate import interp1d #pre written interpolation function\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from scipy import stats #for kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                              162\n",
      "group                        530875\n",
      "date            1985-04-01 00:00:00\n",
      "replace                           1\n",
      "miles                      0.388254\n",
      "replace_next                      1\n",
      "miles_next                 0.388254\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#fix the bus .dat from augirregabiria and Mira's website\n",
    "data = np.fromfile('bus1234.dat')\n",
    "data = data.reshape(len(data)/6,6)\n",
    "data = pd.DataFrame(data,columns=['id','group','year','month','replace','miles'])\n",
    "\n",
    "#save to .csv so other people don't need to be confused\n",
    "data.to_csv(\"bus1234.csv\")\n",
    "\n",
    "#divide by 1e6 (use the same scale are Rust and AM)\n",
    "data['miles'] = (data['miles'])/1e6\n",
    "\n",
    "#switch to date time for ease \n",
    "data['date'] = pd.to_datetime(data[['year', 'month']].assign(Day=1))\n",
    "data = data[['id','group','date','replace','miles']]\n",
    "\n",
    "#lag date\n",
    "date_lag = data.copy()\n",
    "date_lag['date'] = date_lag['date'] - pd.DateOffset(months=1)\n",
    "data = data.merge(date_lag, how='left', on=['id','group','date'] , suffixes=('','_next'))\n",
    "data = data.dropna()\n",
    "\n",
    "print data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "BETA = .9999\n",
    "GAMMA = .5772 #euler's constant\n",
    "\n",
    "#size of step in discretization\n",
    "STEP = .002\n",
    "\n",
    "#make states global variables\n",
    "STATES = np.arange(data['miles'].min(),data['miles'].max() + STEP, STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculating Choice Probabilities 'Non-Parametrically'\n",
    "\n",
    "The next step in our CCP estimation involves estimating the probability of replacing the bus engine with as few assumptions as possible. In particular, calculating our value function requires two objects. \n",
    "\n",
    "1. The transition matrix $F(i)$ \n",
    "2. The conditional replacement probabilities $P$.\n",
    "\n",
    "I estimated these probabilities using the same approach as Aguirregabiria and Mira. However, you can experiment with other (consistent) methods.\n",
    "\n",
    "## The transition matrix\n",
    "\n",
    "We need the amount that each bus's mileage $x$ will increase depending on the engine replacement decision $i$. We can think about this as a $K \\times K$ matrix, where $K$ is the number of states our discretized variable can take. The rows of the matrix refer to the current state $x_t$ and the columns are $x_t$. Let $F(i)$ be the transition matrix between states depending on the replacement decision $i$. We will learn this using the Guassian kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miles_pdf(i_obs, x_obs, x_next):\n",
    "    \"\"\"estimation of mileage pdf following AM using the\n",
    "    kernel function\n",
    "    \n",
    "    this corresponds to pdfdx in AM's code\"\"\"\n",
    "    \n",
    "    #figure out max number of steps\n",
    "    dx = (1-i_obs)*(x_next - x_obs) + i_obs*x_next\n",
    "    \n",
    "    #number of 'transition' states\n",
    "    dx_states = np.arange(dx.min(),dx.max() +STEP , STEP)\n",
    "    \n",
    "    #use kernel groups to make pdf\n",
    "    kernel1 = stats.gaussian_kde(dx, bw_method='silverman')\n",
    "    pdfdx = kernel1(dx_states)\n",
    "    \n",
    "    return np.array([pdfdx/pdfdx.sum()]).transpose()\n",
    "\n",
    "\n",
    "MILES_PDF = miles_pdf(data['replace'], data['miles'], data['miles_next'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_1(i_obs, x_obs , x_next):\n",
    "    \"\"\"calculate transitions probabilities,\n",
    "    non-parametrically\n",
    "    \n",
    "    this corresponds to fmat2 in AM's code\"\"\"\n",
    "    \n",
    "    #transitions when i=1\n",
    "    pdfdx = miles_pdf(i_obs, x_obs, x_next).transpose()\n",
    "    \n",
    "    #zero probability of transitioning to large states\n",
    "    zeros = np.zeros( (len(STATES),len(STATES)-pdfdx.shape[1]) )\n",
    "    \n",
    "    #transitioning to first state and 'jumping' dx states\n",
    "    fmat1 = np.tile(pdfdx,(len(STATES),1))\n",
    "    fmat1 = np.concatenate( (fmat1, zeros), axis=1 )\n",
    "\n",
    "    return fmat1\n",
    "\n",
    "FMAT1 = transition_1(data['replace'], data['miles'],data['miles_next'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_0(i_obs, x_obs, x_next):\n",
    "    \"\"\"calculate transitions probabilities,\n",
    "    non-parametrically\n",
    "    \n",
    "    this corresponds to fmat1 in AM's code\"\"\"\n",
    "    \n",
    "    pdfdx = miles_pdf(i_obs, x_obs, x_next).transpose()\n",
    "    \n",
    "    #initialize fmat array, transitions when i=0\n",
    "    end_zeros = np.zeros((1, len(STATES) - pdfdx.shape[1]))\n",
    "    fmat0 = np.concatenate( (pdfdx, end_zeros), axis=1 )\n",
    "\n",
    "    for row in range(1, len(STATES)):\n",
    "        \n",
    "        #this corresponds to colz i think\n",
    "        cutoff = ( len(STATES) - row - pdfdx.shape[1] )\n",
    "        \n",
    "        #case 1 far enough from the 'end' of the matrix\n",
    "        if cutoff >= 0:\n",
    "            start_zeros = np.zeros((1,row))\n",
    "            end_zeros = np.zeros((1, len(STATES) - pdfdx.shape[1] - row))\n",
    "            fmat_new = np.concatenate( (start_zeros, pdfdx, end_zeros), axis=1 )\n",
    "            fmat0 = np.concatenate((fmat0, fmat_new))\n",
    "       \n",
    "        #case 2, too far from the end and need to adjust probs\n",
    "        else:\n",
    "            pdf_adj = pdfdx[:,0:cutoff]\n",
    "            pdf_adj = pdf_adj/pdf_adj.sum(axis=1)\n",
    "            \n",
    "            start_zeros = np.zeros((1,row))\n",
    "            fmat_new = np.concatenate( (start_zeros, pdf_adj), axis=1 )\n",
    "            fmat0 = np.concatenate((fmat0, fmat_new))\n",
    "            \n",
    "    return fmat0\n",
    "\n",
    "FMAT0 = transition_0(data['replace'],data['miles'],data['miles_next'])\n",
    "\n",
    "PR_TRANS = FMAT0, FMAT1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Choice Probabilities\n",
    "\n",
    "We also need the probability of engine replacement decision $i$ and 'conditional on mileage $x$. Let $P$ be a $K \\times 1$ vector with the probability of replacing the engine conditional on the mileage $x$. We will learn this using a logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036201\n",
      "         Iterations 23\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   No. Observations:                 8156\n",
      "Model:                          Logit   Df Residuals:                     8152\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Tue, 15 Jan 2019   Pseudo R-squ.:                  0.1671\n",
      "Time:                        14:20:03   Log-Likelihood:                -295.26\n",
      "converged:                       True   LL-Null:                       -354.51\n",
      "                                        LLR p-value:                 1.623e-25\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -17.3136      4.188     -4.134      0.000     -25.522      -9.105\n",
      "x1           149.3089     56.675      2.634      0.008      38.228     260.390\n",
      "x2          -553.1128    245.812     -2.250      0.024   -1034.895     -71.330\n",
      "x3           697.5936    340.855      2.047      0.041      29.531    1365.657\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.37 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "def initial_pr(i_obs, x_obs, d=0):\n",
    "    \"\"\"initial the probability of view a given state following AM.\n",
    "    just involves logit to predict\n",
    "    \n",
    "    Third arguement involves display\"\"\"\n",
    "    \n",
    "    X = np.array([x_obs, x_obs**2, x_obs**3]).transpose()\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.Logit(i_obs,X)\n",
    "    fit = model.fit(disp=d)\n",
    "    if d: print fit.summary()\n",
    "    \n",
    "    x_states = np.array([STATES, STATES**2, STATES**3]).transpose()\n",
    "    x_states = sm.add_constant(x_states)\n",
    "    \n",
    "    return fit.predict(x_states)\n",
    "\n",
    "PR_OBS = initial_pr(data['replace'], data['miles'], d=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Alternative Value Function Representation\n",
    "\n",
    "Now that we have non-parametric estimates of the transition matrices and and choice probabilities we can calculate the value function to learn the importance of mileage to the bus manager. \n",
    "\n",
    "The key insight in Rust 1987 and [Pakes 1986](https://www.jstor.org/stable/1912835) is that agents with the same characteristics may make different decisions. In other words, the bus manager may make different decisions about two buses with the same mileage. To accomplish this, agents experience a shock $\\epsilon$ each period based on unobserved costs. To make the problem analytically tractable, we assume that this shock follows an extreme value distribution (so, that the PDF and CDF have the same functional form). Rust also makes the assumption that these shocks are effect decisions like random noise (conditional independece). In other words, the shocks do not systematically influence the mileage.\n",
    "\n",
    "Using the cost function (whose parameters we want to learn), the transition matrices, and choice probabilities we can now calculate the value function from Rust's paper.\n",
    "\n",
    "$$V = [I_m - \\beta[(1-P) \\otimes F(0) + P \\otimes F(1)] ]^{-1} [(1-P)*(u(0,x;\\theta) + \\gamma -ln(1-P)) + P*( u(1,x;\\theta) + \\gamma -ln(P) ) ]$$\n",
    "\n",
    "This corresponds to equation (8) in Aguirregabiria Mira 2002. You can find a formal derivation of this in Hotz Miller 1993 and Aguirregabira Mira's paper.\n",
    "\n",
    "For the purposes of clarifying the formula to see how I implemented it.\n",
    "* $\\otimes$ is the Kroenecker product (i.e. column wise). I implemented this by tiling the vector.\n",
    "* $*$ is the Hadamard produce (i.e. element wise)\n",
    "* $u(i,x;\\theta)$ is cost function. I implemented the cost function using a python `lambda` expression. This means that the routine for calculating the value function takes the cost function (and its parameters) as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hm_value(params, cost, pr_obs, pr_trans):\n",
    "    \"\"\"calculate value function using hotz miller approach\"\"\"\n",
    "    \n",
    "    #set up matrices, transition is deterministic\n",
    "    trans0, trans1 = pr_trans\n",
    "    \n",
    "    #calculate value function for all state\n",
    "    pr_tile = np.tile( pr_obs.reshape( len(STATES) ,1), (1, len(STATES) ))\n",
    "    \n",
    "    denom = (np.identity( len(STATES) ) - BETA*(1-pr_tile)*trans0 - BETA*pr_tile*trans1)\n",
    "    \n",
    "    numer = ( (1-pr_obs)*(cost(params, STATES, 0) + GAMMA - np.log(1-pr_obs)) + \n",
    "                 pr_obs*(cost(params, STATES, 1) + GAMMA - np.log(pr_obs) ) )\n",
    "    \n",
    "    value = np.linalg.inv(denom).dot(numer)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: (Psuedo) Maximum Likelihood Estimaton\n",
    "\n",
    "With the value function we just calculated, we can adjust the likehood of replacing the engine at mileage $x$ with the following formula. This is a $K\\times1$ vector with a probability for each state. This formula is very similiar to the logit we used before. Now, the replacement probability also depends on $\\beta$ and future $x$. \n",
    "\n",
    "$$\\psi(P; \\theta) = \\dfrac{exp[u(1,x,\\theta) + \\beta F(1) V(x)] }{exp[u(1,x,\\theta) + \\beta F(1) V(x)] + exp[u(0,x,\\theta) + \\beta F(0) V(x)] }$$ \n",
    "\n",
    "This corresponds to $\\Psi$ in Aguirregabiria Mira 2002. They parametrize $\\Psi$ using the extreme value distribution right below Proposition 3 in their paper.\n",
    "\n",
    "Ultimately, we wanted to learn how much mileage influences the bus manager's decision.  To estimate the parameters in the value function $\\theta$ (which tell us this), we can maximize the value of this 'adjusted' likehood. This may seem like regular maximum likehood estimation, but adjusting the likelihood with a value function is different. By using information about the choice probabilities, we are 'cheating' to make the routine run faster. By cheating we loose precision (efficiency) in our estimates. To be more efficient, we must calculate the value function without this information. The most common approach involves repeatedly applying the Bellman operator to find the fixed point of the Bellman equation. Our method is quick and still fits the data more accurately than a regular logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hm_prob(params, cost, pr_obs, pr_trans):\n",
    "    \"\"\"calculate kappa (i.e. CCP likelihood) using value function\"\"\"\n",
    "    \n",
    "    value = hm_value(params, cost, pr_obs, pr_trans)\n",
    "    value = value - value.min() #subtract out smallest value\n",
    "    trans0, trans1 = pr_trans\n",
    "    \n",
    "    delta1 = np.exp( cost(params, STATES, 1) + BETA*trans1.dot(value))\n",
    "    delta0 = np.exp( cost(params, STATES, 0) + BETA*trans0.dot(value) )\n",
    "    \n",
    "    return delta1/(delta1+delta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCP(GenericLikelihoodModel):\n",
    "    \"\"\"class for estimating the values of R and theta\n",
    "    using the CCP routine and the helper functions\n",
    "    above\"\"\"\n",
    "    \n",
    "    def __init__(self, i, x, x_next, params, cost, **kwds):\n",
    "        \"\"\"initialize the class\n",
    "        \n",
    "        i - replacement decisions\n",
    "        x - miles\n",
    "        x_next - next periods miles\n",
    "        params - names for cost function parameters\n",
    "        cost - cost function specification, takes agruements (params, x, i) \"\"\"\n",
    "        \n",
    "        super(CCP, self).__init__(i, x, **kwds)\n",
    "        \n",
    "        #data\n",
    "        self.endog = i #these names don't work exactly\n",
    "        self.exog = x #the idea is that x is mean indep of epsilon\n",
    "        self.x_next = x_next\n",
    "        \n",
    "        #transitions\n",
    "        self.pr_obs = initial_pr(i, x)\n",
    "        self.trans =  transition_0(i,x,x_next), transition_1(i,x,x_next)\n",
    "        \n",
    "        #should probably make these class parameters\n",
    "        self.num_states = ( x.max()/STEP).astype(int) + 2\n",
    "        self.states = np.arange(x.min(),x.max() + STEP, STEP)\n",
    "        \n",
    "        #initial model fit\n",
    "        self.cost = cost\n",
    "        self.num_params = len(params)\n",
    "        self.data.xnames =  params\n",
    "        self.results = self.fit( start_params=np.ones(self.num_params) )\n",
    "        \n",
    "        \n",
    "    def nloglikeobs(self, params, v=False):\n",
    "        \"\"\"psuedo log likelihood function for the CCP estimator\"\"\"\n",
    "        \n",
    "        # Input our data into the model\n",
    "        i = self.endog\n",
    "        x = (self.exog/STEP).astype(int)*STEP #discretized x\n",
    "           \n",
    "        #set up hm state pr\n",
    "        prob = hm_prob(params, self.cost, self.pr_obs, self.trans).transpose()\n",
    "        prob = interp1d(self.states, prob)\n",
    "        prob = prob(x)\n",
    "        \n",
    "        log_likelihood = (1-i)*np.log(1-prob) + i*np.log(prob)\n",
    "        \n",
    "        return -log_likelihood.sum()\n",
    "    \n",
    "    \n",
    "    def iterate(self, numiter):\n",
    "        \"\"\"iterate the Hotz Miller estimation procedure 'numiter' times\"\"\"\n",
    "        i = 0\n",
    "        while(i < numiter):\n",
    "            #update pr_obs based on parameters\n",
    "            self.pr_obs = hm_prob(self.results.params, self.cost, self.pr_obs, self.trans)\n",
    "            \n",
    "            #refit the model\n",
    "            self.results = self.fit(start_params=np.ones(self.num_params))\n",
    "            i = i +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036544\n",
      "         Iterations: 63\n",
      "         Function evaluations: 120\n",
      "                                 CCP Results                                  \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   Log-Likelihood:                -298.05\n",
      "Model:                            CCP   AIC:                             598.1\n",
      "Method:            Maximum Likelihood   BIC:                             605.1\n",
      "Date:                Tue, 15 Jan 2019                                         \n",
      "Time:                        14:37:02                                         \n",
      "No. Observations:                8156                                         \n",
      "Df Residuals:                    8155                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta1        -0.5218      0.093     -5.632      0.000      -0.703      -0.340\n",
      "theta2       -10.1309      0.983    -10.301      0.000     -12.058      -8.203\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "#define cost functon using lambda expression\n",
    "LINEAR_COST = lambda params, x, i: (1-i)*x*params[i] + i*params[i]\n",
    "\n",
    "model_ccp = CCP(data['replace'], data['miles'], data['miles_next'], ['theta1','theta2'], LINEAR_COST)\n",
    "print model_ccp.results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Costs\n",
    "\n",
    "We can see that the change in specification does not drastically change the estimates. Considering the limited data, the cost function's parameterization is probability not identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036261\n",
      "         Iterations: 147\n",
      "         Function evaluations: 260\n",
      "                                 CCP Results                                  \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   Log-Likelihood:                -295.75\n",
      "Model:                            CCP   AIC:                             593.5\n",
      "Method:            Maximum Likelihood   BIC:                             600.5\n",
      "Date:                Tue, 15 Jan 2019                                         \n",
      "Time:                        14:35:24                                         \n",
      "No. Observations:                8156                                         \n",
      "Df Residuals:                    8155                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta1        -2.5281      1.088     -2.324      0.020      -4.660      -0.396\n",
      "theta2         4.0832      2.156      1.894      0.058      -0.142       8.308\n",
      "theta3       -15.2180      3.255     -4.675      0.000     -21.598      -8.838\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "QUAD_COST = lambda params, x, i: (1-i)*(x*params[0] + x**2*params[1]) + i*params[2]\n",
    "\n",
    "model_ccp = CCP(data['replace'], data['miles'], data['miles_next'], ['theta1','theta2', 'theta3'], QUAD_COST)\n",
    "print model_ccp.results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Iterating the Model\n",
    "\n",
    "Before we calculated $\\psi(P;\\theta)$ by estimating the probability of replacing the engine using a logit. Our approach to estimating $P$ is very simple. \n",
    "\n",
    "You should be wondering what happens if we use a better starting $P$? In other words, if we use a more precise $P$ will $\\psi(P;\\theta)$ be more precise? Aguirregabiria Mira showed it will be in their 2002 paper! In other words, if we use a better starting $P$ like $\\psi(P;\\hat{\\theta}$ will $\\psi( \\psi(P;\\hat{\\theta}; \\theta))$ will be more precise. In fact, if you keep iterating $\\psi( \\dot ;\\hat{\\theta}$ then $\\psi(\\dot;\\hat{\\theta}$ will converge to 'true' likelihood function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036544\n",
      "         Iterations: 63\n",
      "         Function evaluations: 120\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036530\n",
      "         Iterations: 62\n",
      "         Function evaluations: 117\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036528\n",
      "         Iterations: 63\n",
      "         Function evaluations: 118\n",
      "                                 CCP Results                                  \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   Log-Likelihood:                -297.93\n",
      "Model:                            CCP   AIC:                             597.9\n",
      "Method:            Maximum Likelihood   BIC:                             604.9\n",
      "Date:                Tue, 15 Jan 2019                                         \n",
      "Time:                        14:35:56                                         \n",
      "No. Observations:                8156                                         \n",
      "Df Residuals:                    8155                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta1        -0.5411      0.091     -5.978      0.000      -0.719      -0.364\n",
      "theta2       -10.2082      0.955    -10.692      0.000     -12.080      -8.337\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "model_ccp = CCP(data['replace'], data['miles'], data['miles_next'], ['theta1','theta2'], LINEAR_COST)\n",
    "model_ccp.iterate(2)\n",
    "print model_ccp.results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
