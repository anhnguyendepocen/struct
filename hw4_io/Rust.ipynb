{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional Choice Probabilitiy Estimators in 5 Easy Steps!\n",
    "\n",
    "## Author: Eric Schulman\n",
    "\n",
    "The following guide demonstrates how to use conditional choice probability (CCP) estimators in Python. It was written in part as a homework for the University of Texas second year course in industrial organization. These estimators are the normal way to think about how the future influences decisions in industrial organization and related economic fields.\n",
    "\n",
    "To demonstrate how to use (and implement) a CCP estimator, we recover parameters for the cost function in [Rust 1987](https://www.jstor.org/stable/1911259). Rust's paper considers the decision of a bus manager. The bus manager decides whether or not to replace bus engines for his fleet of buses in Madison, Wisconsion. Replacing the engine has a high cost in the present but letting the engine accumulate mileage makes the bus more likely to break down in the future. Our goal is estimating parameters that tell us the importance of mileage when the bus manager decides to replace the engines. The bus manager's problem is very general and has become the 'mascot' for dynamic decisions in industrial organization.\n",
    "\n",
    "We could use a logit to predict the bus manager's decisions. However, Rust found that a model where agents considered the future predicted bus engine replacement decisions more accurately. To think about the future, Rust solved a dynamic program to calculate the expected value of 'yes' and 'no' based on the current mileage. Rust included this value function inside a logit. His estimation routine alternates between finding the Bellman operator's fixed point and estimating the logit with a maximum likelihood routine. This approach is called the nested fixed point algorithm (NFXP). Rust has code for solving this value function and estimate parameters in Gauss on his [website](https://editorialexpress.com/jrust/nfxp.html). Additionally, there are [tutorials](https://nbviewer.jupyter.org/github/QuantEcon/QuantEcon.notebooks/blob/master/ddp_ex_rust96_py.ipynb) demonstrating how to find the fixed point of the Bellman operator in Python.\n",
    "\n",
    "The more recent approach to predicting dynamic choices is called conditional choice probability (CCP) estimation. This approach works similar to Rust's assymptotically. However, it simplifies Rust's NFXP. Instead of embedding a value function into a logit, we start with a simple estimate of the choice probabilities and adjust this estimate to account for the future. Your initial estimate of the choice probabilities will provide an estimate of the value function. This estimated value function will help adjust the estimates of how mileage influences the replacement probability. This approach was first introduced to the literature in [Hotz Miller 1993](https://www.jstor.org/stable/2298122). The code and data I used for this guide from comes from Victor Aguirregabiria and Pedro Mira's [website](http://individual.utoronto.ca/vaguirre/wpapers/program_code_survey_joe_2008.html) accompanying their paper [Aguirregabiria Mira 2002](http://individual.utoronto.ca/vaguirre/wpapers/program_code_survey_joe_2008.html) (more on them later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Pre-processing the data and structural constants\n",
    "\n",
    "There are two important factors involved with setting up the data\n",
    "1. First, we  must pick a discount factor. This is the most important aspect of the CCP estimator and distinguishes it from a logit. Implicitly, our choice is an assumption about the bus manager because nothing in the data tells us about agent's discount factor (for more about this see [Magnac Thesmar 2002](https://www.jstor.org/stable/2692293)). All we see are mileage and replacment decisions. Making this assumption better explains the data.\n",
    "\n",
    "2. Calculating the value function involves framing the bus manager's problem as a Markov process. As a result, we discretize our continuous data on mileage to caclulate the value function for a given amount of mileage more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.interpolate import interp1d #pre written interpolation function\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "from scipy import stats #for kernel function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                              162\n",
      "group                        530875\n",
      "date            1985-04-01 00:00:00\n",
      "replace                           1\n",
      "miles                      0.388254\n",
      "replace_next                      1\n",
      "miles_next                 0.388254\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#format the bus .dat from augirregabiria and mira's website\n",
    "data = np.fromfile('bus1234.dat')\n",
    "data = data.reshape(len(data)/6,6)\n",
    "data = pd.DataFrame(data,columns=['id','group','year','month','replace','miles'])\n",
    "\n",
    "#save to .csv\n",
    "data.to_csv('bus1234.csv')\n",
    "\n",
    "#divide by 1e6 (use the same scale are Rust and AM)\n",
    "data['miles'] = (data['miles'])/1e6\n",
    "\n",
    "#switch to date time for ease \n",
    "data['date'] = pd.to_datetime(data[['year', 'month']].assign(Day=1))\n",
    "data = data[['id','group','date','replace','miles']]\n",
    "\n",
    "#lag date\n",
    "date_lag = data.copy()\n",
    "date_lag['date'] = date_lag['date'] - pd.DateOffset(months=1)\n",
    "data = data.merge(date_lag, how='left', on=['id','group','date'] , suffixes=('','_next'))\n",
    "data = data.dropna()\n",
    "\n",
    "print(data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "BETA = .9999\n",
    "GAMMA = .5772 #euler's constant\n",
    "\n",
    "#size of step in discretization\n",
    "STEP = .002\n",
    "\n",
    "#make states global variables\n",
    "STATES = np.arange(data['miles'].min(),data['miles'].max() + STEP, STEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculating Choice Probabilities 'Non-Parametrically'\n",
    "\n",
    "The next step in our CCP estimation involves estimating the probability of replacing the bus engine with as few assumptions as possible. In particular, calculating our value function requires two objects. \n",
    "\n",
    "1. The transition matrix $F(i)$ \n",
    "2. The conditional replacement probabilities $P$.\n",
    "\n",
    "We estimate these probabilities using the same approach as Aguirregabiria and Mira. However, in principle we could experiment with other (consistent) methods.\n",
    "\n",
    "### The Transition Matrix\n",
    "\n",
    "We need the amount that each bus's mileage $x$ will increase depending on the engine replacement decision $i$. We can think about this as a $K \\times K$ matrix, where $K$ is the number of states our discretized variable can take. The rows of the matrix refer to the current state $x_t$ and the columns are $x_{t+1}$. Let $F(i)$ be the transition matrix between states depending on the replacement decision $i$. We will learn this using the Gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def miles_pdf(i_obs, x_obs, x_next):\n",
    "    \"\"\"estimation of mileage pdf following AM using the\n",
    "    kernel function\n",
    "    \n",
    "    this corresponds to pdfdx in AM's code\"\"\"\n",
    "    \n",
    "    #figure out max number of steps\n",
    "    dx = (1-i_obs)*(x_next - x_obs) + i_obs*x_next\n",
    "    \n",
    "    #number of 'transition' states\n",
    "    dx_states = np.arange(dx.min(),dx.max() +STEP , STEP)\n",
    "    \n",
    "    #use kernel groups to make pdf\n",
    "    kernel1 = stats.gaussian_kde(dx, bw_method='silverman')\n",
    "    pdfdx = kernel1(dx_states)\n",
    "    \n",
    "    return np.array([pdfdx/pdfdx.sum()]).transpose()\n",
    "\n",
    "\n",
    "MILES_PDF = miles_pdf(data['replace'], data['miles'], data['miles_next'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_1(i_obs, x_obs , x_next):\n",
    "    \"\"\"calculate transitions probabilities,\n",
    "    non-parametrically\n",
    "    \n",
    "    this corresponds to fmat2 in AM's code\"\"\"\n",
    "    \n",
    "    #transitions when i=1\n",
    "    pdfdx = miles_pdf(i_obs, x_obs, x_next).transpose()\n",
    "    \n",
    "    #zero probability of transitioning to large states\n",
    "    zeros = np.zeros( (len(STATES),len(STATES)-pdfdx.shape[1]) )\n",
    "    \n",
    "    #transitioning to first state and 'jumping' dx states\n",
    "    fmat1 = np.tile(pdfdx,(len(STATES),1))\n",
    "    fmat1 = np.concatenate( (fmat1, zeros), axis=1 )\n",
    "\n",
    "    return fmat1\n",
    "\n",
    "FMAT1 = transition_1(data['replace'], data['miles'],data['miles_next'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_0(i_obs, x_obs, x_next):\n",
    "    \"\"\"calculate transitions probabilities,\n",
    "    non-parametrically\n",
    "    \n",
    "    this corresponds to fmat1 in AM's code\"\"\"\n",
    "    \n",
    "    pdfdx = miles_pdf(i_obs, x_obs, x_next).transpose()\n",
    "    \n",
    "    #initialize fmat array, transitions when i=0\n",
    "    end_zeros = np.zeros((1, len(STATES) - pdfdx.shape[1]))\n",
    "    fmat0 = np.concatenate( (pdfdx, end_zeros), axis=1 )\n",
    "\n",
    "    for row in range(1, len(STATES)):\n",
    "        \n",
    "        #this corresponds to colz i think\n",
    "        cutoff = ( len(STATES) - row - pdfdx.shape[1] )\n",
    "        \n",
    "        #case 1 far enough from the 'end' of the matrix\n",
    "        if cutoff >= 0:\n",
    "            start_zeros = np.zeros((1,row))\n",
    "            end_zeros = np.zeros((1, len(STATES) - pdfdx.shape[1] - row))\n",
    "            fmat_new = np.concatenate( (start_zeros, pdfdx, end_zeros), axis=1 )\n",
    "            fmat0 = np.concatenate((fmat0, fmat_new))\n",
    "       \n",
    "        #case 2, too far from the end and need to adjust probs\n",
    "        else:\n",
    "            pdf_adj = pdfdx[:,0:cutoff]\n",
    "            pdf_adj = pdf_adj/pdf_adj.sum(axis=1)\n",
    "            \n",
    "            start_zeros = np.zeros((1,row))\n",
    "            fmat_new = np.concatenate( (start_zeros, pdf_adj), axis=1 )\n",
    "            fmat0 = np.concatenate((fmat0, fmat_new))\n",
    "            \n",
    "    return fmat0\n",
    "\n",
    "FMAT0 = transition_0(data['replace'],data['miles'],data['miles_next'])\n",
    "\n",
    "PR_TRANS = FMAT0, FMAT1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Choice Probabilities\n",
    "\n",
    "We also need the probability of engine replacement decision $i$ conditional on mileage $x$. Let $P$ be a $K \\times 1$ vector with the probability of replacing the engine conditional on the mileage $x$. We will learn this using a logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036201\n",
      "         Iterations 23\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   No. Observations:                 8156\n",
      "Model:                          Logit   Df Residuals:                     8152\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Sun, 20 Jan 2019   Pseudo R-squ.:                  0.1671\n",
      "Time:                        14:30:51   Log-Likelihood:                -295.26\n",
      "converged:                       True   LL-Null:                       -354.51\n",
      "                                        LLR p-value:                 1.623e-25\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -17.3136      4.188     -4.134      0.000     -25.522      -9.105\n",
      "x1           149.3089     56.675      2.634      0.008      38.228     260.390\n",
      "x2          -553.1128    245.812     -2.250      0.024   -1034.895     -71.330\n",
      "x3           697.5936    340.855      2.047      0.041      29.531    1365.657\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.37 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "def initial_pr(i_obs, x_obs, d=0):\n",
    "    \"\"\"initial the probability of view a given state following AM.\n",
    "    just involves logit to predict\n",
    "    \n",
    "    Third arguement involves display\"\"\"\n",
    "    \n",
    "    X = np.array([x_obs, x_obs**2, x_obs**3]).transpose()\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    model = sm.Logit(i_obs,X)\n",
    "    fit = model.fit(disp=d)\n",
    "    if d: print(fit.summary())\n",
    "    \n",
    "    x_states = np.array([STATES, STATES**2, STATES**3]).transpose()\n",
    "    x_states = sm.add_constant(x_states)\n",
    "    \n",
    "    return fit.predict(x_states)\n",
    "\n",
    "PR_OBS = initial_pr(data['replace'], data['miles'], d=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Alternative Value Function Representation\n",
    "\n",
    "Now that we have estimates of the transition matrices and and choice probabilities we can calculate the value function to learn the importance of mileage to the bus manager. \n",
    "\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "The key insight in Rust 1987 and [Pakes 1986](https://www.jstor.org/stable/1912835) is that agents with the same characteristics may make different decisions. In other words, the bus manager may make different decisions about two buses with the same mileage. To accomplish this, agents experience a shock $\\epsilon$ each period based on unobserved costs. To make the problem analytically tractable, we usally make 2 important assumptions about $\\epsilon$\n",
    "\n",
    "1. That this shock follows an extreme value distribution (so, that the PDF and CDF have the same functional form). \n",
    "2. We also make the assumption that these shocks are effect decisions like random noise (conditional independece). In other words, the shocks do not systematically influence the mileage.\n",
    "\n",
    "### Calculation\n",
    "\n",
    "Using the cost function (whose parameters we want to learn), the transition matrices, and choice probabilities (which we estimated) we can now calculate the value function from Rust's paper.\n",
    "\n",
    "$$V = [I_m - \\beta( (1-P) * F(0) +  P * F(1)) ]^{-1} [(1-P)*(u(0,x;\\theta) + \\gamma -ln(1-P)) + P*( u(1,x;\\theta) + \\gamma -ln(P) ) ]$$\n",
    "\n",
    "This corresponds to equation (8) in Aguirregabiria Mira 2002. You can find a formal derivation of this in Hotz Miller 1993 and Aguirregabira Mira's paper.\n",
    "\n",
    "For the purposes of clarifying the formula to see how I implemented it.\n",
    "* $*$ is the Hadamard produce (i.e. element wise).\n",
    "* $u(i,x;\\theta)$ is the payoff function. I implemented this function using a Python `lambda` expression so that the cost specification is flexible. This means that the routine for calculating the value function takes the cost function (and its parameters) as an argument. Generally speaking it has the form.\n",
    "\n",
    "* Finally, note there is a slight abuse of notation going on. The dimensions of $F$ are $K \\times K$ so I needed to tile the vector $P$ in order to take the element wise product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hm_value(params, cost, pr_obs, pr_trans):\n",
    "    \"\"\"calculate value function using hotz miller approach\"\"\"\n",
    "    \n",
    "    #set up matrices, transition is deterministic\n",
    "    trans0, trans1 = pr_trans\n",
    "    \n",
    "    #calculate value function for all state\n",
    "    pr_tile = np.tile( pr_obs.reshape( len(STATES) ,1), (1, len(STATES) ))\n",
    "    \n",
    "    denom = (np.identity( len(STATES) ) - BETA*(1-pr_tile)*trans0 - BETA*pr_tile*trans1)\n",
    "    \n",
    "    numer = ( (1-pr_obs)*(cost(params, STATES, 0) + GAMMA - np.log(1-pr_obs)) + \n",
    "                 pr_obs*(cost(params, STATES, 1) + GAMMA - np.log(pr_obs) ) )\n",
    "    \n",
    "    value = np.linalg.inv(denom).dot(numer)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: (Psuedo) Maximum Likelihood Estimaton\n",
    "\n",
    "With the value function we just calculated, we can adjust the likehood of replacing the engine at mileage $x$ with the following formula. It is very similiar to the logit we used before and produces a $K\\times1$ vector with a probability of replacement in each state. Now, the replacement probability also depends on $\\beta$ and future $x$'s. \n",
    "\n",
    "$$\\psi(P ; \\theta) = \\dfrac{exp[u(1,x,\\theta) + \\beta F(1) V] }{exp[u(1,x,\\theta) + \\beta F(1) V] + exp[u(0,x,\\theta) + \\beta F(0) V] }$$ \n",
    "\n",
    "This corresponds to the $\\Psi$ function in Aguirregabiria Mira 2002. They parametrize $\\Psi$ using the extreme value distribution right below Proposition 3 in their paper.\n",
    "\n",
    "Ultimately, we wanted to learn how mileage influences the bus manager's decision by estimating the parameters in the cost function $\\theta$. To learn these parameters, we can maximize the value of this likehood $\\psi$ 'adjusted' for the  value function we estimated. This differs from regular maximum likehood estimation. This approach 'cheated' by estimating the value ahead of time to make the routine run faster. By cheating we loose precision (efficiency) in our estimates. \n",
    "\n",
    "To do true maximum likelihood estimation, we must formally calculate the value function (instead of estimating it using the data). The most common approach for calculating the value functon involves repeatedly applying the Bellman operator to find the fixed point of the Bellman equation. Our method is quick and still fits the data more accurately than a regular logit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hm_prob(params, cost, pr_obs, pr_trans):\n",
    "    \"\"\"calculate psi (i.e. CCP likelihood) using \n",
    "    the value function from the hotz miller appraoch\"\"\"\n",
    "    \n",
    "    value = hm_value(params, cost, pr_obs, pr_trans)\n",
    "    value = value - value.min() #subtract out smallest value\n",
    "    trans0, trans1 = pr_trans\n",
    "    \n",
    "    delta1 = np.exp( cost(params, STATES, 1) + BETA*trans1.dot(value))\n",
    "    delta0 = np.exp( cost(params, STATES, 0) + BETA*trans0.dot(value) )\n",
    "    \n",
    "    return delta1/(delta1+delta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCP(GenericLikelihoodModel):\n",
    "    \"\"\"class for estimating the values of R and theta\n",
    "    using the CCP routine and the helper functions\n",
    "    above\"\"\"\n",
    "    \n",
    "    def __init__(self, i, x, x_next, params, cost, **kwds):\n",
    "        \"\"\"initialize the class\n",
    "        \n",
    "        i - replacement decisions\n",
    "        x - miles\n",
    "        x_next - next periods miles\n",
    "        params - names for cost function parameters\n",
    "        cost - cost function specification, takes agruements (params, x, i) \"\"\"\n",
    "        \n",
    "        super(CCP, self).__init__(i, x, **kwds)\n",
    "        \n",
    "        #data\n",
    "        self.endog = i #these names don't work exactly\n",
    "        self.exog = x #the idea is that x is mean indep of epsilon\n",
    "        self.x_next = x_next\n",
    "        \n",
    "        #transitions\n",
    "        self.pr_obs = initial_pr(i, x)\n",
    "        self.trans =  transition_0(i,x,x_next), transition_1(i,x,x_next)\n",
    "        \n",
    "        #should probably make these class parameters\n",
    "        self.num_states = ( x.max()/STEP).astype(int) + 2\n",
    "        self.states = np.arange(x.min(),x.max() + STEP, STEP)\n",
    "        \n",
    "        #initial model fit\n",
    "        self.cost = cost\n",
    "        self.num_params = len(params)\n",
    "        self.data.xnames =  params\n",
    "        self.results = self.fit( start_params=np.ones(self.num_params) )\n",
    "        \n",
    "        \n",
    "    def nloglikeobs(self, params, v=False):\n",
    "        \"\"\"psuedo log likelihood function for the CCP estimator\"\"\"\n",
    "        \n",
    "        # Input our data into the model\n",
    "        i = self.endog\n",
    "        x = (self.exog/STEP).astype(int)*STEP #discretized x\n",
    "           \n",
    "        #set up hm state pr\n",
    "        prob = hm_prob(params, self.cost, self.pr_obs, self.trans).transpose()\n",
    "        prob = interp1d(self.states, prob)\n",
    "        prob = prob(x)\n",
    "        \n",
    "        log_likelihood = (1-i)*np.log(1-prob) + i*np.log(prob)\n",
    "        \n",
    "        return -log_likelihood.sum()\n",
    "    \n",
    "    \n",
    "    def iterate(self, numiter):\n",
    "        \"\"\"iterate the Hotz Miller estimation procedure 'numiter' times\"\"\"\n",
    "        i = 0\n",
    "        while(i < numiter):\n",
    "            #update pr_obs based on parameters\n",
    "            self.pr_obs = hm_prob(self.results.params, self.cost, self.pr_obs, self.trans)\n",
    "            \n",
    "            #refit the model\n",
    "            self.results = self.fit(start_params=np.ones(self.num_params))\n",
    "            i = i +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Costs\n",
    "\n",
    "First we estimate the cost of additional mileage and the cost of replacing a new engine using a linear cost function specification. This means that the cost associated with driving a bus is directly proportional to its mileage. \n",
    "\n",
    "$$c(x;\\theta) = \\theta_1 x$$\n",
    "\n",
    "Augirregabiria and Mira found the 'true' maximum likelihood estimates for these parameters in their paper which I included below for reference. Our CCP estimator preforms relatively well compared to these parameter estimates. \n",
    "\n",
    "|Parameter| MLE Estimate |\n",
    "|--|--|\n",
    "| $\\theta_1$|-.58 |\n",
    "|$RC$| -10.47|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036544\n",
      "         Iterations: 63\n",
      "         Function evaluations: 120\n",
      "                                 CCP Results                                  \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   Log-Likelihood:                -298.05\n",
      "Model:                            CCP   AIC:                             598.1\n",
      "Method:            Maximum Likelihood   BIC:                             605.1\n",
      "Date:                Sun, 20 Jan 2019                                         \n",
      "Time:                        14:32:44                                         \n",
      "No. Observations:                8156                                         \n",
      "Df Residuals:                    8155                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta1        -0.5218      0.093     -5.632      0.000      -0.703      -0.340\n",
      "RC           -10.1309      0.983    -10.301      0.000     -12.058      -8.203\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "#define cost functon using lambda expression\n",
    "linear_cost = lambda params, x, i: (1-i)*x*params[i] + i*params[i]\n",
    "\n",
    "linear_model = CCP(data['replace'], data['miles'], data['miles_next'], ['theta1','RC'], linear_cost)\n",
    "print(linear_model.results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Costs\n",
    "\n",
    "Rust wanted to demonstrate that a model where the Bus manager considered the future better fit the data. As a result, Rust gave a very flexible for his cost function. One aditional specification involves quadratic costs.\n",
    "\n",
    "$$c(x;\\theta) = \\theta_1 x + \\theta_2 x^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036261\n",
      "         Iterations: 147\n",
      "         Function evaluations: 260\n",
      "                                 CCP Results                                  \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   Log-Likelihood:                -295.75\n",
      "Model:                            CCP   AIC:                             593.5\n",
      "Method:            Maximum Likelihood   BIC:                             600.5\n",
      "Date:                Sun, 20 Jan 2019                                         \n",
      "Time:                        14:33:04                                         \n",
      "No. Observations:                8156                                         \n",
      "Df Residuals:                    8155                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta1        -2.5281      1.088     -2.324      0.020      -4.660      -0.396\n",
      "theta2         4.0832      2.156      1.894      0.058      -0.142       8.308\n",
      "R            -15.2180      3.255     -4.675      0.000     -21.598      -8.838\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "quad_cost = lambda params, x, i: (1-i)*(x*params[0] + x**2*params[1]) + i*params[2]\n",
    "\n",
    "quad_model = CCP(data['replace'], data['miles'], data['miles_next'], ['theta1','theta2', 'R'], quad_cost)\n",
    "print(quad_model.results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Cost Functions\n",
    "\n",
    "When we compare the value of a given amount of mileage under both specifications side by side, we see that the change does not drastically change the value at a given mileage. Considering the limited data, we probably cannot learn the actual functional form for the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4dc424afd0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#set up plot of both value functions\n",
    "fig = plt.figure()\n",
    "v_linear = hm_value(linear_model.results.params, linear_model.cost, linear_model.pr_obs, linear_model.trans) \n",
    "v_linear = v_linear - v_linear.min() #normalize\n",
    "v_quad = hm_value(quad_model.results.params, quad_model.cost, quad_model.pr_obs, quad_model.trans)\n",
    "v_quad = v_quad - v_quad.min() #normalize\n",
    "\n",
    "#make a plot of both value functions\n",
    "plt.ylabel('Value Function')\n",
    "plt.xlabel('Mileage (divided by 10e6)')\n",
    "plt.plot(STATES,v_linear,label='Linear')\n",
    "plt.plot(STATES,v_quad, label='Quadratic')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Iterating the Model\n",
    "\n",
    "Before we calculated $\\psi(P;\\theta)$ by estimating the probability of replacing the engine using a logit. The key to calculating  $\\psi( \\cdot ;\\theta)$ is $P$. Our approach to estimating $P$ is very simple. So, you may wonder what happens if we use a better starting estimate of the choice probabilities? If we use a more precise $P$, Aguirregabiria and Mira showed that $\\psi(P;\\theta)$ will be more precise in their 2002 paper! \n",
    "\n",
    "Using better starting value like $\\psi(P;\\hat{\\theta})$ guarantees that $\\psi( \\psi(P;\\hat{\\theta}); \\theta)$ will be more precise. What's more, nothing is stopping you from repeating this process. After caclulating $\\psi( \\psi(P;\\hat{\\theta}); \\theta)$, you can plug your estimate back into $\\psi( \\cdot ; \\theta)$. In fact, if you keep iterating then you will converge to 'true' likelihood function estimates.\n",
    "\n",
    "To demonstrate how this works, we iterate the CCP estimation procedure by repeatedly plugging the previous estimate of $\\psi( \\cdot ; \\theta)$ into the likelhood function. We can see that the estimates get closer to Aguirregabiria and Mira's 'true' maximum likelihood estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036528\n",
      "         Iterations: 60\n",
      "         Function evaluations: 115\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.036528\n",
      "         Iterations: 59\n",
      "         Function evaluations: 114\n",
      "                                 CCP Results                                  \n",
      "==============================================================================\n",
      "Dep. Variable:                replace   Log-Likelihood:                -297.93\n",
      "Model:                            CCP   AIC:                             597.9\n",
      "Method:            Maximum Likelihood   BIC:                             604.9\n",
      "Date:                Sun, 20 Jan 2019                                         \n",
      "Time:                        14:42:52                                         \n",
      "No. Observations:                8156                                         \n",
      "Df Residuals:                    8155                                         \n",
      "Df Model:                           0                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta1        -0.5406      0.090     -6.022      0.000      -0.717      -0.365\n",
      "RC           -10.2035      0.951    -10.726      0.000     -12.068      -8.339\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "linear_model.iterate(2)\n",
    "print(linear_model.results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
