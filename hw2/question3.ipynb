{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 Structural Econometrics: Question 3\n",
    "## November 9, 2018 \n",
    "## Eric Schulman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.gmm import GMM\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into memory\n",
    "data = pd.DataFrame(data = np.genfromtxt('sim3.dat', delimiter='  '), columns=['i','t','y_t','p_t'])\n",
    "\n",
    "#set up lag\n",
    "shift_data = data[['i','y_t','p_t']]\n",
    "shift_data['t'] = data['t'] + 1\n",
    "data = data.merge(shift_data,how='left',on=['i','t'],suffixes=['','-1'])\n",
    "data = data.fillna(0) # the initial period is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "\n",
    "\n",
    "Because of state dependence, $U_{it}$ only depends on $\\epsilon_{it}$ when we know the values of $p_{it}, y_{it-1}$, and $\\alpha_i$. Since, $\\epsilon_{it}$ is i.i.d.\n",
    "\n",
    "$$Pr(\\textbf{1}(U_{it} >0)| p_{it}, y_{it-1}; \\theta) \\perp Pr(\\textbf{1}(U_{it+1} >0)| p_{it+1}, y_{it}; \\theta) $$ \n",
    "\n",
    "Since $y_{it} = \\textbf{1}(U_{it} >0)$\n",
    "\n",
    "$$Pr(y_{it}| p_{it}, y_{it-1}, \\alpha_i; \\theta) \\perp Pr( y_{it+1}| p_{it+1}, y_{it}, \\alpha_i; \\theta)$$\n",
    "\n",
    "\n",
    "Thus we can write \n",
    "\n",
    "$$\\int Pr( y_{i1},... y_{iT} | p_{i1}, ... ,p_{it}, y_{i0} , \\alpha_i; \\theta ) d \\alpha'_i = $$\n",
    "\n",
    "$$\\int Pr( \\textbf{1}(U_{i1} >0), ... , \\textbf{1}(U_{iT} >0) | p_{i1}, ... ,p_{iT}, y_{i0}, .... y_{i,T},  \\alpha_i ; \\theta ) d \\alpha'_i = $$\n",
    "\n",
    "$$\\int Pr(\\textbf{1}(U_{i1} >0)| p_{i1}, y_{i0}, \\alpha_i) ... Pr(\\textbf{1}(U_{iT} >0)| p_{iT}, y_{iT-1}, \\alpha_i ; \\theta)  d \\alpha'_i = $$\n",
    "\n",
    "\n",
    "\n",
    "$$\\int Pr(y_{i1} | p_{i1}, y_{i0}, \\alpha_i) ... Pr(y_{iT} | p_{iT}, y_{iT-1},  \\alpha_i ; \\theta)  d \\alpha'_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "\n",
    "The equality would not hold without integrating out $\\alpha_i$.  The $\\epsilon_{it}$ may be correlated over time due to individual heterogeneity. The terms inside the product are no longer independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "\n",
    "According to our model\n",
    "\n",
    "$$Pr(y_{it} | \\alpha_i, y_{it-1}, p_{it}) =Pr(\\textbf{1}(U_{it} >0)| p_{it}, y_{it-1})$$ \n",
    "\n",
    "\n",
    "When $y_{it} = 1$, we know that $U_{it} >0$, so \n",
    "\n",
    "$$Pr(\\textbf{1}(U_{it} >0)| p_{it}, y_{it-1}) = \\dfrac{e^{\\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\sigma_\\alpha \\alpha_i } }{ 1 + e^{\\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\sigma_\\alpha \\alpha_i}}$$\n",
    "\n",
    "When $y_{it} = 0 $, our model tells us that $U_{it} <0$ so  \n",
    "\n",
    "$$ Pr(\\textbf{1}(U_{it} >0)| p_{it}, y_{it-1}) = \\dfrac{1 }{ 1 + e^{\\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\sigma_\\alpha \\alpha_i}}$$\n",
    "\n",
    "Combining these results we know\n",
    "\n",
    "$$Pr(y_{it} | \\alpha_i, y_{it-1}, p_{it}) = y_{it} \\dfrac{e^{\\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\sigma_\\alpha \\alpha_i } }{ 1 + e^{\\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\sigma_\\alpha \\alpha_i}} + (1-y_{it})  \\dfrac{1 }{ 1 + e^{\\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\sigma_\\alpha \\alpha_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up useful global variables\n",
    "\n",
    "NSIM = 1000\n",
    "T = int(data.groupby('i').count().max()['t'])\n",
    "I = len(data.i.unique())\n",
    "\n",
    "alpha = np.random.normal(0,1 ,(NSIM, I))\n",
    "alpha = alpha.reshape( (1, NSIM, I) )\n",
    "alpha = np.tile(alpha, (T, 1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_data(x):\n",
    "    \"\"\" format data as a 3d array to make working with it easier\"\"\"\n",
    "    x = np.array([x])\n",
    "    x = x.reshape(I,1,T)\n",
    "    x = np.tile(x ,(1,NSIM,1)).transpose() \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.620089\n",
      "         Iterations: 157\n",
      "         Function evaluations: 257\n",
      "                                part_d Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                    y_t   Log-Likelihood:                -1240.2\n",
      "Model:                         part_d   AIC:                             2484.\n",
      "Method:            Maximum Likelihood   BIC:                             2496.\n",
      "Date:                Thu, 08 Nov 2018                                         \n",
      "Time:                        19:31:55                                         \n",
      "No. Observations:                2000                                         \n",
      "Df Residuals:                    1998                                         \n",
      "Df Model:                           1                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta_0       -0.8067      0.209     -3.852      0.000      -1.217      -0.396\n",
      "theta_1        0.9111      0.179      5.100      0.000       0.561       1.261\n",
      "theta_2        0.5013      0.109      4.588      0.000       0.287       0.715\n",
      "sigma          0.8801      0.097      9.094      0.000       0.690       1.070\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "class part_d(GenericLikelihoodModel):\n",
    "    \"\"\"class for evaluating question 3 part d\"\"\"\n",
    "    \n",
    "    def __init__(self, sims, *args, **kwds):\n",
    "        super(part_d, self).__init__(*args, **kwds)\n",
    "        self.sims = sims\n",
    "    \n",
    "    \n",
    "    def nloglikeobs(self, params):\n",
    "        \"\"\"Log likelihood function derived above\"\"\"\n",
    "        t0, t1, t2, sigma = params\n",
    "        y = shape_data(self.endog)\n",
    "        \n",
    "        p = shape_data(self.exog.transpose()[0])\n",
    "        y_lag = shape_data(self.exog.transpose()[1])\n",
    "        alpha = self.sims\n",
    "        \n",
    "        #calculate the mean 'delta' for the inside good\n",
    "        U1 = np.exp(t0 + t1*p + t2*y_lag + sigma*alpha)\n",
    "     \n",
    "        #calculate ll, for each simulation\n",
    "        like =  y*U1/(1.+U1) + (1.-y)/(1.+U1)\n",
    "        like =  1./NSIM * (like.prod(axis=0)).sum(axis=0)\n",
    "        like = np.log(like).sum(axis = 0)\n",
    "\n",
    "        return - like\n",
    "\n",
    "    \n",
    "    def fit(self, start_params=None, maxiter=1000, maxfun=5000, **kwds):\n",
    "        \"\"\"fit log likelihood to data\"\"\"\n",
    "        if start_params == None:\n",
    "            start_params = [.5,.5,.5,.5]\n",
    "        return super(part_d, self).fit(start_params=start_params,\n",
    "                                       maxiter=maxiter, maxfun=maxfun, **kwds)\n",
    "    \n",
    "\n",
    "model_d = part_d(alpha, data['y_t'] ,data[['p_t','y_t-1']])\n",
    "\n",
    "result_d = model_d.fit()\n",
    "print(result_d.summary(xname=['theta_0', 'theta_1', 'theta_2', 'sigma']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part e\n",
    "\n",
    "The coefficient on $\\theta_2$ and $\\sigma_\\alpha$ are both significant. The coefficient on $\\sigma_\\alpha$ is larger than $\\theta_2$. As a result, individual heterogeniety may be more important in explaining the correlation acrros time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part f\n",
    "\n",
    "By re-estimating the model (below) the coefficient on $\\theta_2$ increases. This is individual heterogenity is now an omitted variable which is correlated with the effect from the preivous state. As a result, excluding the heterogeniety causes an upward bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.646619\n",
      "         Iterations: 94\n",
      "         Function evaluations: 164\n",
      "                                part_f Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                    y_t   Log-Likelihood:                -1293.2\n",
      "Model:                         part_f   AIC:                             2590.\n",
      "Method:            Maximum Likelihood   BIC:                             2602.\n",
      "Date:                Wed, 07 Nov 2018                                         \n",
      "Time:                        17:16:39                                         \n",
      "No. Observations:                2000                                         \n",
      "Df Residuals:                    1998                                         \n",
      "Df Model:                           1                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta_0       -0.9485      0.176     -5.390      0.000      -1.293      -0.604\n",
      "theta_1        0.7298      0.163      4.475      0.000       0.410       1.049\n",
      "theta_2        1.0118      0.094     10.768      0.000       0.828       1.196\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "class part_f(GenericLikelihoodModel):\n",
    "    \"\"\"class for evaluating question 3 part f\"\"\"\n",
    "    \n",
    "    def nloglikeobs(self, params, v=False):\n",
    "        t0, t1, t2 = params\n",
    "        y = self.endog\n",
    "        p,y_lag = self.exog.transpose()\n",
    "    \n",
    "        #calculate the mean 'delta' for the inside good\n",
    "        U1 = t0 + t1*p + t2*y_lag\n",
    "        U1 = np.exp(U1)\n",
    "        \n",
    "        #calculate ll, for each simulation\n",
    "        likelihood_sims = np.log(y*U1/(1+U1) + (1-y)/(1+U1))\n",
    "        likelihood = likelihood_sims.sum(axis=0)\n",
    "        \n",
    "        if v: raise Exception('Stop drop and roll')\n",
    "        return - likelihood.sum()\n",
    "\n",
    "    \n",
    "    def fit(self, start_params=None, maxiter=1000, maxfun=5000, **kwds):\n",
    "        if start_params == None:\n",
    "            start_params = [.5,.5,.5]\n",
    "        return super(part_f, self).fit(start_params=start_params,\n",
    "                                       maxiter=maxiter, maxfun=maxfun, **kwds)\n",
    "    \n",
    "\n",
    "model_f = part_f(data['y_t'],data[['p_t','y_t-1']])\n",
    "\n",
    "result_f = model_f.fit()\n",
    "print(result_f.summary(xname=['theta_0', 'theta_1', 'theta_2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crude way to test the null hypothesis would be using the following linear probability model\n",
    "\n",
    "$$y_{it} = \\theta_1 p_{it} + \\theta_2 p_{it-1} \\epsilon_{it}$$\n",
    "\n",
    "We could run a t-test on $\\theta_2$ to test for state dependence.\n",
    "\n",
    "There are technically two key exclusionary restriction here. First we need price is exogenous. Secondly, we require $p_{it-1}$ only effects $y_{it}$ through the current state $y_{it-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
