{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.gmm import GMM\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data into memory\n",
    "data = pd.DataFrame(data = np.genfromtxt('sim3.dat', delimiter='  '), columns=['i','t','y_t','p_t'])\n",
    "\n",
    "#set up lag\n",
    "shift_data = data[['i','y_t','p_t']]\n",
    "shift_data['t'] = data['t'] + 1\n",
    "data = data.merge(shift_data,how='left',on=['i','t'],suffixes=['','-1'])\n",
    "data = data.fillna(0) # the initial period is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part a\n",
    "\n",
    "\n",
    "Because of state dependence, $U_{it}$ is deterministic when we know the values of $p_{it}, y_{it-1}$, and $\\alpha_i$ (at least according to our model). As a result\n",
    "\n",
    "$$Pr(\\textbf{1}(U_{it} >0)| p_{it}, y_{it-1}) \\perp Pr(\\textbf{1}(U_{it+1} >0)| p_{it+1}, y_{it}) $$ so,\n",
    "\n",
    "Since $y_{it} = \\textbf{1}(U_{it} >0)$\n",
    "\n",
    "$$Pr(y_{it}| p_{it}, y_{it-1}, \\alpha_i) \\perp Pr( y_{it+1}| p_{it+1}, y_{it}, \\alpha_i)$$\n",
    "\n",
    "\n",
    "Thus we can write \n",
    "\n",
    "$$\\int Pr( y_{i1},... y_{iT} | p_{i1}, ... ,p_{it}, y_{i0} , \\alpha_i; \\theta ) d \\alpha'_i = $$\n",
    "\n",
    "$$\\int Pr( \\textbf{1}(U_{i1} >0), ... , \\textbf{1}(U_{iT} >0) | p_{i1}, ... ,p_{it}, y_{i0}, .... y_{i,t-1}, , \\alpha_i ; \\theta ) d \\alpha'_i = $$\n",
    "\n",
    "$$\\int Pr(\\textbf{1}(U_{i1} >0)| p_{i1}, y_{i0}, \\alpha_i) ... Pr(\\textbf{1}(U_{iT} >0)| p_{iT}, y_{iT-1}, , \\alpha_i)  d \\alpha'_i = $$\n",
    "\n",
    "\n",
    "\n",
    "$$\\int Pr(y_{i1} | p_{i1}, y_{i0}, \\alpha_i) ... Pr(y_{iT} | p_{iT}, y_{iT-1}, , \\alpha_i)  d \\alpha'_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part b\n",
    "\n",
    "It would not be true without integrating out $\\alpha_i$.  Now the $\\epsilon_{it}$ may be correlated over time due to individual heterogeneity. The terms inside the product are no longer independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part c\n",
    "\n",
    "$$Pr(y_{it} | \\alpha_i, y_{it-1}, p_{it}) =Pr(\\textbf{1}(U_{it} >0)| p_{it}, y_{it-1})$$ according to our model\n",
    "\n",
    "\n",
    "When $y_{it} = 1$ we know that $U_{it} >0$, so \n",
    "\n",
    "$$Pr(\\textbf{1}(U_{it} >0)| p_{it}, y_{it-1}) = \\dfrac{e^{\\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\sigma_\\alpha \\alpha_i } }{ 1 + e^{\\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\sigma_\\alpha \\alpha_i}}$$\n",
    "\n",
    "When $y_{it} = 0 $ we know that $U_{it} <0$ so  \n",
    "\n",
    "$$ Pr(\\textbf{1}(U_{it} >0)| p_{it}, y_{it-1}) = \\dfrac{1 }{ 1 + e^{\\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\sigma_\\alpha \\alpha_i}}$$\n",
    "\n",
    "So,\n",
    "\n",
    "$$Pr(y_{it} | \\alpha_i, y_{it-1}, p_{it}) = y_{it} \\dfrac{e^{\\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\sigma_\\alpha \\alpha_i } }{ 1 + e^{\\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\sigma_\\alpha \\alpha_i}} + (1-y_{it})  \\dfrac{1 }{ 1 + e^{\\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\sigma_\\alpha \\alpha_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up useful global variables\n",
    "\n",
    "NSIM = 1000\n",
    "T = int(data.groupby('i').count().max()['t'])\n",
    "I = len(data.i.unique())\n",
    "\n",
    "alpha = np.random.normal(0,1 ,(NSIM, I))\n",
    "alpha = alpha.reshape( (1, NSIM, I) )\n",
    "alpha = np.tile(alpha, (T, 1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1000, 100)\n"
     ]
    }
   ],
   "source": [
    "def shape_data(x):\n",
    "    \"\"\" format data as a 3d array to make working with it easier\"\"\"\n",
    "    x = np.array([x])\n",
    "    x = x.reshape(I,1,T)\n",
    "    x = np.tile(x ,(1,NSIM,1)).transpose() \n",
    "    return x\n",
    "\n",
    "print shape_data(data['y_t']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.620078\n",
      "         Iterations: 146\n",
      "         Function evaluations: 256\n",
      "                                part_d Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                    y_t   Log-Likelihood:                -1240.2\n",
      "Model:                         part_d   AIC:                             2484.\n",
      "Method:            Maximum Likelihood   BIC:                             2496.\n",
      "Date:                Wed, 07 Nov 2018                                         \n",
      "Time:                        17:28:57                                         \n",
      "No. Observations:                2000                                         \n",
      "Df Residuals:                    1998                                         \n",
      "Df Model:                           1                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta_0       -0.7901      0.210     -3.766      0.000      -1.201      -0.379\n",
      "theta_1        0.9132      0.179      5.113      0.000       0.563       1.263\n",
      "theta_2        0.5012      0.109      4.588      0.000       0.287       0.715\n",
      "sigma          0.8771      0.096      9.117      0.000       0.689       1.066\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "class part_d(GenericLikelihoodModel):\n",
    "    \"\"\"class for evaluating question 3 part d\"\"\"\n",
    "    \n",
    "    def __init__(self, sims, *args, **kwds):\n",
    "        # set appropriate counts for moment conditions and parameters\n",
    "        super(part_d, self).__init__(*args, **kwds)\n",
    "        self.sims = sims\n",
    "    \n",
    "    \n",
    "    def nloglikeobs(self, params, v=False):\n",
    "        t0, t1, t2, sigma = params\n",
    "        y = shape_data(self.endog)\n",
    "        \n",
    "        p = shape_data(self.exog.transpose()[0])\n",
    "        y_lag = shape_data(self.exog.transpose()[1])\n",
    "        alpha = self.sims\n",
    "        \n",
    "        #calculate the mean 'delta' for the inside good\n",
    "        U1 = np.exp(t0 + t1*p + t2*y_lag + sigma*alpha)\n",
    "     \n",
    "        #calculate ll, for each simulation\n",
    "        like =  y*U1/(1+U1) + (1-y)/(1+U1)\n",
    "        like =  1./NSIM * (like.prod(axis=0)).sum(axis=0)\n",
    "        like = np.log(like).sum(axis = 0)\n",
    "\n",
    "        return - like\n",
    "\n",
    "    \n",
    "    def fit(self, start_params=None, maxiter=1000, maxfun=5000, **kwds):\n",
    "        if start_params == None:\n",
    "            start_params = [-.798,.5,.5,.5]\n",
    "        return super(part_d, self).fit(start_params=start_params,\n",
    "                                       maxiter=maxiter, maxfun=maxfun, **kwds)\n",
    "    \n",
    "\n",
    "model_d = part_d(alpha, data['y_t'] ,data[['p_t','y_t-1']])\n",
    "\n",
    "result_d = model_d.fit()\n",
    "print(result_d.summary(xname=['theta_0', 'theta_1', 'theta_2', 'sigma']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part e\n",
    "\n",
    "The coefficient on $\\theta_2$ and $\\sigma_\\alpha$ are both significant. However, the coefficient on $\\sigma_\\alpha$ is larger than $\\theta_2$. As a result, individual heterogeniety may be more important in explaining the correlation acrros time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part f\n",
    "\n",
    "By re-estimating the model (below) the coefficient on $\\theta_2$ increases. This is individual heterogenity is now an omitted variable which is correlated with the preivous state. As a result, excluding the heterogeniety causes an upward bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.646619\n",
      "         Iterations: 94\n",
      "         Function evaluations: 164\n",
      "                                part_f Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                    y_t   Log-Likelihood:                -1293.2\n",
      "Model:                         part_f   AIC:                             2590.\n",
      "Method:            Maximum Likelihood   BIC:                             2602.\n",
      "Date:                Wed, 07 Nov 2018                                         \n",
      "Time:                        17:16:39                                         \n",
      "No. Observations:                2000                                         \n",
      "Df Residuals:                    1998                                         \n",
      "Df Model:                           1                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta_0       -0.9485      0.176     -5.390      0.000      -1.293      -0.604\n",
      "theta_1        0.7298      0.163      4.475      0.000       0.410       1.049\n",
      "theta_2        1.0118      0.094     10.768      0.000       0.828       1.196\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "class part_f(GenericLikelihoodModel):\n",
    "    \"\"\"class for evaluating question 3 part f\"\"\"\n",
    "    \n",
    "    def nloglikeobs(self, params, v=False):\n",
    "        t0, t1, t2 = params\n",
    "        y = self.endog\n",
    "        p,y_lag = self.exog.transpose()\n",
    "    \n",
    "        #calculate the mean 'delta' for the inside good\n",
    "        U1 = t0 + t1*p + t2*y_lag\n",
    "        U1 = np.exp(U1)\n",
    "        \n",
    "        #calculate ll, for each simulation\n",
    "        likelihood_sims = np.log(y*U1/(1+U1) + (1-y)/(1+U1))\n",
    "        likelihood = likelihood_sims.sum(axis=0)\n",
    "        \n",
    "        if v: raise Exception('Stop drop and roll')\n",
    "        return - likelihood.sum()\n",
    "\n",
    "    \n",
    "    def fit(self, start_params=None, maxiter=1000, maxfun=5000, **kwds):\n",
    "        if start_params == None:\n",
    "            start_params = [-.948,.5,.5]\n",
    "        return super(part_f, self).fit(start_params=start_params,\n",
    "                                       maxiter=maxiter, maxfun=maxfun, **kwds)\n",
    "    \n",
    "\n",
    "model_f = part_f(data['y_t'],data[['p_t','y_t-1']])\n",
    "\n",
    "result_f = model_f.fit()\n",
    "print(result_f.summary(xname=['theta_0', 'theta_1', 'theta_2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crude way to test the null hypothesis would be using the following linear probability model\n",
    "\n",
    "$$y_{it} = \\theta_0 + \\theta_1 p_{it} + \\theta_2 y_{it-1} + \\alpha_i  + \\epsilon_{it}$$\n",
    "\n",
    "Here $\\alpha_i$ is a fixed effect to capture the individual specific heterogenity.\n",
    "\n",
    "We could run a t-test on $\\theta_2$ to test for state dependence.\n",
    "\n",
    "The exlcusion restriction would be that $\\alpha_i$ and $y_{it-1}$ are uncorrelated. If $\\alpha_i$ determined $y_{it-1}$ then the coefficient would be biased upward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
