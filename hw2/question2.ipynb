{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.gmm import GMM\n",
    "from statsmodels.base.model import GenericLikelihoodModel\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y      0.568393\n",
      "x1    42.537849\n",
      "x2    12.286853\n",
      "z      9.029880\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#load data into memory\n",
    "data = pd.DataFrame(data = np.genfromtxt('ps2.dat', delimiter='  '), columns=['y','x1','x2','z'])\n",
    "\n",
    "print data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part a\n",
    "\n",
    "An economic story where $x_{2i}$ is correlated with $\\epsilon_i$ involves simultaneity between the decision of an education level and years they want to work. Women who intend to stay in the labor force longer may have select more education."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part c\n",
    "\n",
    "$\\rho$ represents the correlation between the two error terms. The intrument is relevant it should be non-zero. Since, one would expect parents education to be positively related to your education, you would expect $\\rho$ to be positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part d\n",
    "\n",
    "In order to estimate the model we must derive the likelihood function.\n",
    "\n",
    "$p(y_i,x_{2i} | x_{1i}, z_i, \\theta) = p(x_{2i} | x_{1i}, z_i, \\theta) p(y_i | x_{1i},x_{2i}, z_i, \\theta)  = p(x_{2i} | x_{1i}, z_i, \\theta) p(y_i | x_{1i},x_{2i}, z_i, \\eta_i, \\theta)$\n",
    "\n",
    "1. Preforming a change of variable of $\\eta_i$ for $x_{2i}$, we can write\n",
    "\n",
    "$p(x_{2i} | x_{1i}, z_i, \\theta) = p(\\eta_i|x_i,z_i,\\theta)\\dfrac{dx_{2i}}{d\\eta_i} = \\phi(\\dfrac{\\eta_i}{\\sigma})  \\dfrac{1}{\\sigma}$\n",
    "\n",
    "2. We can derive an analytic experession for  $p(y_i | x_{1i},x_{2i}, z_i, \\theta)$ below\n",
    "\n",
    "$p(y_i | x_{1i},x_{2i}, z_i, \\theta)$\n",
    "\n",
    "When $y_i =1$ we have, $p(y_i | x_{1i},x_{2i}, z_i, \\theta) = E(\\textbf{1}(\\epsilon_i + \\theta_2 \\eta_i + \\theta_0 + \\theta_2\\theta_3 + (\\theta_1 + \\theta_2\\theta_4) + \\theta_2\\theta_5 z_i > 0 ) | \\eta_i ) =  1 - P(\\epsilon_i + \\gamma_i > 0) $\n",
    "\n",
    "For notational convenience we have let, $\\gamma_i = \\theta_2 \\eta_i + \\theta_0 + \\theta_2\\theta_3 + (\\theta_1 + \\theta_2\\theta_4) + \\theta_2\\theta_5 z_i$\n",
    "\n",
    "When $y_i = 0$ we have, $p(y_i | x_{1i},x_{2i}, z_i, \\theta) = E(\\textbf{1}(\\epsilon_i + \\theta_2 \\eta_i + \\theta_0 + \\theta_2\\theta_3 + (\\theta_1 + \\theta_2\\theta_4) + \\theta_2\\theta_5 z_i < 0 ) | \\eta_i ) =  P(\\epsilon_i + \\gamma_i > 0) $\n",
    "\n",
    "So, we have $p(y_i | x_{1i},x_{2i}, z_i, \\theta) = (1-y_i) P(\\epsilon_i + \\gamma_i > 0) + y_i (1 - P(\\epsilon_i + \\gamma_i > 0) ) $\n",
    "\n",
    "Using results about the distribution of conditional normals we know\n",
    "\n",
    "$\\epsilon_i|\\eta_i \\sim N(\\eta_i \\dfrac{\\rho}{\\sigma_\\eta^2}, 1 - \\dfrac{\\rho^2}{\\sigma_\\eta^2})$\n",
    "\n",
    "\n",
    "So, $p(y_i | x_{1i},x_{2i}, z_i, \\theta) = y_i (1 -\\Phi(\\dfrac{- \\gamma_i - \\frac{\\rho}{\\sigma_\\eta^2}}{1 - \\frac{\\rho^2}{\\sigma_\\eta^2}})) + (1-y_i)\\Phi(\\dfrac{- \\gamma_i - \\frac{\\rho}{\\sigma_\\eta^2}}{1 - \\frac{\\rho^2}{\\sigma_\\eta^2}}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     x2   R-squared:                       0.245\n",
      "Model:                            OLS   Adj. R-squared:                  0.243\n",
      "Method:                 Least Squares   F-statistic:                     121.7\n",
      "Date:                Sat, 27 Oct 2018   Prob (F-statistic):           1.63e-46\n",
      "Time:                        16:22:05   Log-Likelihood:                -1582.8\n",
      "No. Observations:                 753   AIC:                             3172.\n",
      "Df Residuals:                     750   BIC:                             3185.\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          9.1255      0.493     18.507      0.000       8.157      10.093\n",
      "x1            -0.0031      0.009     -0.341      0.733      -0.021       0.015\n",
      "z              0.3649      0.024     15.137      0.000       0.318       0.412\n",
      "==============================================================================\n",
      "Omnibus:                       14.540   Durbin-Watson:                   1.961\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               26.395\n",
      "Skew:                           0.037   Prob(JB):                     1.86e-06\n",
      "Kurtosis:                       3.914   Cond. No.                         302.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model_d_stage1 = sm.OLS(data['x2'],sm.add_constant(data[['x1','z']]) )\n",
    "model_d_stage1_fit = model_d_stage1.fit()\n",
    "T3, T4, T5 = model_d_stage1_fit.params #initialize these\n",
    "\n",
    "print model_d_stage1_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3.256370\n",
      "         Iterations: 761\n",
      "         Function evaluations: 1159\n",
      "                                part_d Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:            ['y', 'x2']   Log-Likelihood:                -2452.0\n",
      "Model:                         part_d   AIC:                             4908.\n",
      "Method:            Maximum Likelihood   BIC:                             4917.\n",
      "Date:                Sat, 27 Oct 2018                                         \n",
      "Time:                        16:38:00                                         \n",
      "No. Observations:                 753                                         \n",
      "Df Residuals:                     751                                         \n",
      "Df Model:                           1                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "theta_0       -0.3436      0.655     -0.524      0.600      -1.628       0.941\n",
      "theta_1       -0.0100      0.006     -1.672      0.094      -0.022       0.002\n",
      "theta_2        0.0771      0.044      1.767      0.077      -0.008       0.163\n",
      "theta_3        9.4311      0.348     27.069      0.000       8.748      10.114\n",
      "theta_4       -0.0096      0.006     -1.477      0.140      -0.022       0.003\n",
      "theta_5        0.3615      0.017     21.229      0.000       0.328       0.395\n",
      "rho            0.0752      0.096      0.781      0.435      -0.114       0.264\n",
      "sigma          1.4010      0.026     54.802      0.000       1.351       1.451\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "class part_d(GenericLikelihoodModel):\n",
    "    \"\"\"class for evaluating question 1 part b\"\"\"\n",
    "    \n",
    "    def nloglikeobs(self, params, v=False):\n",
    "        \n",
    "        t0,t1,t2,t3,t4,t5,rho,sigma = params\n",
    "\n",
    "        y,x2 = self.endog.transpose()\n",
    "        x1,z = self.exog.transpose()\n",
    "        \n",
    "        eta = x2 - t3 - t4*x1 - t5*z\n",
    "        \n",
    "        mu_epsilon = (rho/sigma**2)*eta\n",
    "        var_epsilon = np.sqrt(abs(1 - (rho/sigma)**2))\n",
    "        \n",
    "        #pr(eta | ... )\n",
    "        recalc_eta = x2 - t3 - t4*x1 - t5*z\n",
    "        pr_eta = norm(0,sigma).pdf(recalc_eta)/sigma\n",
    "        \n",
    "        #pr(y|x2 ... )\n",
    "        gamma = t0 + t2*t3 + (t1 + t2*t4)*x1 + t2*t5*z + t2*eta\n",
    "        \n",
    "        pr_epsilon = (y*(1 - norm(mu_epsilon,var_epsilon).cdf(-gamma))\n",
    "                      + (1-y)*norm(mu_epsilon,var_epsilon).cdf(-gamma))\n",
    "        #if v: print pr_epsilon.max(), pr_epsilon.min(), pr_epsilon.mean(), '\\n'\n",
    "        \n",
    "        #likelihood = np.log( pr_eta * np.maximum(pr_epsilon,.0001) )\n",
    "        likelihood = np.log( pr_epsilon*pr_eta )\n",
    "\n",
    "        \n",
    "        if v: raise Exception('Stop drop and roll')\n",
    "        \n",
    "        return -( likelihood.sum() ) \n",
    "    \n",
    "    \n",
    "    def fit(self, start_params=None, maxiter=2000, maxfun=5000, **kwds):\n",
    "        # we have one additional parameter and we need to add it for summary\n",
    "        if start_params == None:\n",
    "            start_params = [-.391,.3,.2,T3, T4, T5 ,.12,1.33]\n",
    "            \n",
    "        return super(part_d, self).fit(start_params=start_params,\n",
    "                                       maxiter=maxiter, maxfun=maxfun, **kwds)\n",
    "\n",
    "    \n",
    "model_d = part_d(data[['y','x2']],data[['x1','z']])\n",
    "\n",
    "result_d = model_d.fit()\n",
    "print(result_d.summary(xname=['theta_0', 'theta_1', 'theta_2',\n",
    "                              'theta_3','theta_4','theta_5',\n",
    "                              'rho', 'sigma']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
